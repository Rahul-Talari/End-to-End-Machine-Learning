{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f83ed5",
   "metadata": {},
   "source": [
    "# Dataframes using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f972abe",
   "metadata": {},
   "source": [
    "1. Create DF from file\n",
    "2. Handling Missing, Duplicate, and Sorted Data \n",
    "3. Understanding data\n",
    "4. Column Transformation\n",
    "5. Filter & select data \n",
    "6. Merge, Groupby, pivoting\n",
    "6. save Dataframe to File "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7a9fe",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 14px; margin-bottom: 6px;\">1. Creating DF</h3>\n",
    "\n",
    "<div style=\"font-size: 12px; width: 100%; overflow-x: auto;\">\n",
    "  <table border=\"1\" cellpadding=\"4\" cellspacing=\"0\" style=\"border-collapse: collapse;\">\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th>Category</th>\n",
    "        <th>Examples</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td><b>Flat files</b></td>\n",
    "        <td><code>read_csv</code>, <code>read_table</code>, <code>read_fwf</code></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Excel / Clipboard</b></td>\n",
    "        <td><code>read_excel</code>, <code>read_clipboard</code></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Columnar / Binary formats</b></td>\n",
    "        <td><code>read_parquet</code>, <code>read_feather</code>, <code>read_orc</code>, <code>read_hdf</code>, <code>read_pickle</code></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>JSON / Web data</b></td>\n",
    "        <td><code>read_json</code>, <code>read_html</code>, <code>read_xml</code></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>SQL / Databases</b></td>\n",
    "        <td><code>read_sql</code>, <code>read_sql_query</code>, <code>read_sql_table</code></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Python data structures</b></td>\n",
    "        <td><code>pd.DataFrame(dict)</code>, <code>pd.DataFrame(list)</code>, <code>pd.DataFrame(np.array)</code></td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401fd55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# 1. Flat Files\n",
    "df = pd.read_csv(\"file.csv\")                             # CSV\n",
    "df_table = pd.read_table(\"file.txt\")                         # use when data is separated by whitespace (like TSV)\n",
    "df_fwf = pd.read_fwf(\"fixed width file.txt\")                 #  use when data is aligned by column width, with no separators.\n",
    "\n",
    "# 2. Excel Files\n",
    "df_excel = pd.read_excel(\"file.xlsx\", sheet_name=\"Sheet1\")   # Excel files, specify sheet name if needed\n",
    "df_excel_all = pd.read_excel(\"file.xlsx\", sheet_name=None)   # Load all sheets into a dictionary of DataFrames   \n",
    "df_clipboard = pd.read_clipboard()                           # Copy data from clipboard, useful for quick pasting\n",
    "\n",
    "#3. Columnar formats\n",
    "df_parquet = pd.read_parquet(\"file.parquet\")                 # Parquet files, efficient for large datasets\n",
    "df_orc = pd.read_orc(\"file.orc\")                             # ORC files,\n",
    "df_feather = pd.read_feather(\"file.feather\")                 # Feather files, fast binary format\n",
    "df_pickle = pd.read_pickle(\"file.pkl\")                       # Pickle files, Python-specific serialization\n",
    "df_hdf = pd.read_hdf(\"file.h5\")                              # HDF5 files, hierarchical data format\n",
    "\n",
    "# 4. JSON/web data\n",
    "df_json = pd.read_json(\"file.json\")                          # JSON files\n",
    "df_html = pd.read_html(\"file.html\")                          # HTML tables, returns a list of DataFrames\n",
    "df_xml = pd.read_xml(\"file.xml\")                             # XML files, returns a DataFrame\n",
    "\n",
    "# 5. SQL databases\n",
    "conn = sqlite3.connect(\"database.db\")                                                        # SQLite database connection\n",
    "df_sql_query = pd.read_sql_query(\"SELECT * FROM table_name\", conn)                           # To execute custom SQL queries.\n",
    "df_sql_table = pd.read_sql_table(\"table_name\", conn)                                         # To load entire table\n",
    "df_sql = pd.read_sql(\"SELECT * FROM table_name\", conn)                                       # General SQL query execution\n",
    "df_sql_gbq = pd.read_gbq(\"SELECT * FROM dataset.table_name\", project_id=\"your_project_id\")   # Google BigQuery\n",
    "\n",
    "# 6. Python data structures\n",
    "data_dict = {\"col1\": [1, 2], \"col2\": [3, 4]}\n",
    "df_dict = pd.DataFrame(data_dict)                            # From a dictionary\n",
    "data_list = [[1, 2], [3, 4]]\n",
    "df_list = pd.DataFrame(data_list, columns=[\"col1\", \"col2\"])  # From a list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9d30d",
   "metadata": {},
   "source": [
    "2. Handling Missing, Duplicate, and Sorted Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffdae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting DataFrame\n",
    "df.sort_values(['col1', 'col2'], ascending=[1, 1])         # 0=False, 1=True\n",
    "\n",
    "# Handling Missing Data\n",
    "df.isnull()                                                # returns a DataFrame of boolean values indicating missing data\n",
    "df.isnull().sum()                                          # returns a Series with the count of missing values per column\n",
    "df.dropna()                                                # drops rows with any missing values\n",
    "df.fillna('default_value')                                 # fills missing values with a specified value\n",
    "\n",
    "# Handling Duplicate Data\n",
    "df.duplicated()                                            # returns a Series of boolean values indicating duplicate rows\n",
    "df.duplicated().sum()                                      # returns the count of duplicate rows\n",
    "df.drop_duplicates()                                       # drops duplicate rows\n",
    "df.drop_duplicates(subset=['col1', 'col2'], keep='first')  # drop duplicates based on specific columns, keeping the first occurrence\n",
    "df.drop_duplicates(subset=['col1', 'col2'], keep=False)    # drop all duplicates, keeping none\n",
    "\n",
    "# Unique & Frequency\n",
    "df['column1'].unique()                                     # returns unique values in a Series\n",
    "df['column1'].nunique()                                    # returns the count of unique values in a Series\n",
    "df['column1'].value_counts()                               # returns a Series with counts of unique values in a Series\n",
    "df['column1'].sort_values()                                # returns a Series sorted by values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b86c85",
   "metadata": {},
   "source": [
    "3. Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3138ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape                      # returns a tuple (rows, columns)\n",
    "df.columns                    # returns column names\n",
    "df.dtypes                     # returns data types of each column\n",
    "\n",
    "df.head('n')                    # returns first n rows\n",
    "df.tail('n')                    # returns last n rows\n",
    "df.sample('n')                  # returns random n rows\n",
    "df.describe()                 # summary statistics of numeric columns\n",
    "df.info()                     # concise summary: types, non-null counts\n",
    "df.memory_usage()             # memory usage of each column (in bytes)\n",
    "df.memory_usage(deep=True)    # includes deep memory for object types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8301d",
   "metadata": {},
   "source": [
    "4. Column transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformation\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)                     # Reset index and drop the old index\n",
    "df.set_index('col1', inplace=True)                          # Set 'col1' as the new index\n",
    "df.rename(columns={'old_name': 'new_name'}, inplace=True)   # Rename column\n",
    "df = df.astype({'col1': 'int', 'col2': 'float'})            # Convert column data types\n",
    "\n",
    "# âž• Create or update columns\n",
    "df['new_col'] = df['col1'] + df['col2']                     # Create new column by adding two columns\n",
    "df['new_col'] = df['col1'].apply(lambda x: x * 2)           # Apply function to column\n",
    "\n",
    "# âŒ Drop columns\n",
    "df.drop(columns=['col1', 'col2'], inplace=True)             # Drop multiple columns\n",
    "\n",
    "# ðŸ”Ž Filter/select specific columns\n",
    "df_filtered = df.filter(items=['col1', 'col2'])             # Select specific columns\n",
    "\n",
    "\n",
    "\n",
    "df['new_col'] = df['col1'].str.upper()                      # string operation on a column\n",
    "df['new_col'] = df['col1'].str.split(',')                   # split string into list\n",
    "df['new_col'] = df['col1'].str.replace('old', 'new')        # replace substring in a string column\n",
    "df['new_col'] = df['col1'].str.contains('substring')         # check if substring exists in a string column\n",
    "df['new_col'] = df['col1'].str.len()                         # get length of\n",
    "df['new_col'] = df['col1'].str.strip()                       # remove leading/trailing whitespace\n",
    "df['new_col'] = df['col1'].str.split().str[0]                  # split string and get first element\n",
    "df['new_col'] = df['col1'].str.cat(df['col2'], sep=' ')     # concatenate two string columns with a separator\n",
    "df['new_col'] = df['col1'].str.extract(r'(\\d+)')                   # extract digits from a string column using regex\n",
    "df['new_col'] = df['col1'].str.contains('pattern', case=False)  # check if pattern exists in a string column, case-insensitive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02aa08e",
   "metadata": {},
   "source": [
    "5. Filter & select data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Label-based indexing using df.loc[] (stop is inclusive)\n",
    "# âœ… Position-based indexing using df.iloc[] (stop is exclusive)\n",
    "# Note: loc uses row/column labels; iloc uses integer positions\n",
    "\n",
    "# ----------------------------------\n",
    "# ðŸ”¹ Selection using df.loc[]\n",
    "# ----------------------------------\n",
    "\n",
    "# ðŸ‘‰ Row selection\n",
    "df.loc['row_start':'row_end']                         # Select a range of rows\n",
    "df.loc[['row1', 'row2']]                              # Select specific rows\n",
    "\n",
    "# ðŸ‘‰ Column selection (for all rows)\n",
    "df.loc[:, 'col_start':'col_end']                      # Select a range of columns\n",
    "df.loc[:, ['col1', 'col2']]                           # Select specific columns\n",
    "\n",
    "# ðŸ‘‰ Row + Column selection\n",
    "df.loc['row_start':'row_end', 'col_start':'col_end']  # Select range of rows and columns\n",
    "df.loc['row_start':'row_end', ['col1', 'col2']]       # Select rows with specific columns\n",
    "df.loc[['row1', 'row2'], ['col1', 'col2']]            # Specific rows & columns\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# ðŸ”¹ Selection using df.iloc[]\n",
    "# ----------------------------------\n",
    "\n",
    "# ðŸ‘‰ Row selection\n",
    "df.iloc[0:5]                                          # First 5 rows (0 to 4)\n",
    "\n",
    "# ðŸ‘‰ Column selection (for all rows)\n",
    "df.iloc[:, 1:4]                                       # Columns 1 to 3\n",
    "\n",
    "# ðŸ‘‰ Row + Column selection\n",
    "df.iloc[0:5, 1:3]                                     # Rows 0 to 4, Columns 1 to 2\n",
    "df.iloc[[0, 2, 4], [1, 3]]                            # Specific rows & columns\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# ðŸ”¹ Filtering with Conditions (using df.loc)\n",
    "# ----------------------------------\n",
    "\n",
    "filtered_1 = df.loc[df[\"Age\"] > 30]                                  # Age > 30\n",
    "filtered_2 = df.loc[df[\"Name\"].str.contains(\"sh\")]                  # Name contains 'sh'\n",
    "filtered_3 = df.loc[df[\"Name\"].str.startswith(\"sh\")]                # Name starts with 'sh'\n",
    "filtered_4 = df.loc[df[\"Name\"].str.endswith(\"ya\")]                  # Name ends with 'ya'\n",
    "filtered_5 = df.loc[(df[\"Age\"] > 30) & (df[\"Name\"].str.startswith(\"A\"))]  # Combined filter\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# ðŸ”¹ Conditional Changes (using df.loc)\n",
    "# ----------------------------------\n",
    "\n",
    "df.loc[df[\"Age\"] > 30, \"Status\"] = \"Senior\"                         # Assign new column based on Age\n",
    "df.loc[df[\"Name\"].str.startswith(\"A\"), \"Group\"] = \"Alpha\"          # Assign based on Name\n",
    "df.loc[(df[\"Age\"] < 18) & (df[\"Gender\"] == \"F\"), \"Type\"] = \"Minor Girl\"  # Multi-condition update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7a8a1",
   "metadata": {},
   "source": [
    "6. Merge, groupby, pivot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”— Concatenate DataFrames\n",
    "df_concat = pd.concat(['df1', 'df2'], axis=0, ignore_index=True)    # Vertically concatenate rows (stack)\n",
    "df_concat = pd.concat(['df1', 'df2'], axis=1)                        # Horizontally concatenate columns (side by side)\n",
    "\n",
    "# ðŸ”€ Merge DataFrames\n",
    "df_merge = pd.merge('df1', 'df2', on='key_column', how='inner')     # Merge on key column with inner join\n",
    "df_merge = pd.merge('df1', 'df2', how='left', on='key_column')      # Left join\n",
    "df_merge = pd.merge('df1', 'df2', how='outer', on='key_column')     # Full outer join\n",
    "df_merge = pd.merge('df1', 'df2', left_on='key1', right_on='key2')  # Merge on different column names\n",
    "\n",
    "# ðŸ”¢ Group By Operations\n",
    "df_grouped = df.groupby('group_column').sum()                                              # Group by a column and compute sum\n",
    "df_grouped = df.groupby('group_column').mean()                                             # Group and compute mean\n",
    "df_grouped = df.groupby('group_column').agg({'col1': 'sum', 'col2': 'mean'})               # Group and apply multiple aggregations\n",
    "\n",
    "# ðŸ” Pivot Table\n",
    "df_pivot = df.pivot(                                             # Reshape without aggregation\n",
    "    index='index_column',                                        # Values become row indices\n",
    "    columns='column_to_pivot',                                   # Values become column headers\n",
    "    values='value_column'                                        # Fill values from this column\n",
    ")\n",
    "\n",
    "# ðŸ” Pivot Table with Aggregation\n",
    "df_pivot_table = pd.pivot_table(                                 # Reshape with aggregation\n",
    "    data=df,\n",
    "    index='index_column',                                        # Rows\n",
    "    columns='column_to_pivot',                                   # Columns\n",
    "    values='value_column',                                       # Cell values\n",
    "    aggfunc='sum',                                               # Aggregation function: sum, mean, count, etc.\n",
    "    fill_value=0                                                 # Replace NaNs with 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88ab91",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 14px; margin-bottom: 6px;\">7. save Dataframe to File</h3>\n",
    "\n",
    "<div style=\"font-size: 12px; width: 100%; overflow-x: auto;\">\n",
    "  <table border=\"1\" cellpadding=\"4\" cellspacing=\"0\" style=\"border-collapse: collapse;\">\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th>Category</th>\n",
    "        <th>Methods (File Extensions)</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td><b>Clipboard / Files</b></td>\n",
    "        <td>\n",
    "          <code>to_clipboard</code> (no file), \n",
    "          <code>to_csv</code> <i>(.csv)</i>, \n",
    "          <code>to_excel</code> <i>(.xlsx, .xls)</i>, \n",
    "          <code>to_string</code> <i>(.txt, console)</i>\n",
    "        </td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Columnar / Binary formats</b></td>\n",
    "        <td>\n",
    "          <code>to_parquet</code> <i>(.parquet)</i>, \n",
    "          <code>to_feather</code> <i>(.feather)</i>, \n",
    "          <code>to_orc</code> <i>(.orc)</i>, \n",
    "          <code>to_hdf</code> <i>(.h5, .hdf5)</i>, \n",
    "          <code>to_pickle</code> <i>(.pkl)</i>\n",
    "        </td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Markup / Web formats</b></td>\n",
    "        <td>\n",
    "          <code>to_html</code> <i>(.html)</i>, \n",
    "          <code>to_json</code> <i>(.json)</i>, \n",
    "          <code>to_xml</code> <i>(.xml)</i>, \n",
    "          <code>to_latex</code> <i>(.tex)</i>, \n",
    "          <code>to_markdown</code> <i>(.md)</i>\n",
    "        </td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Databases / Cloud</b></td>\n",
    "        <td>\n",
    "          <code>to_sql</code> <i>(database)</i>, \n",
    "          <code>to_gbq</code> <i>(Google BigQuery)</i>, \n",
    "          <code>to_stata</code> <i>(.dta)</i>\n",
    "        </td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Conversion to other formats</b></td>\n",
    "        <td>\n",
    "          <code>to_dict</code> (Python dict), \n",
    "          <code>to_numpy</code> (NumPy array), \n",
    "          <code>to_records</code> (structured NumPy), \n",
    "          <code>to_xarray</code> (Xarray object)\n",
    "        </td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Date/Time conversion</b></td>\n",
    "        <td>\n",
    "          <code>to_period</code>, \n",
    "          <code>to_timestamp</code>, \n",
    "          <code>tz_convert</code>\n",
    "        </td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8b4dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
