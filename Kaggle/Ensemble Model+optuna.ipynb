{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with:**\n",
    "\n",
    "1. *XGBoost + Optuna*\n",
    "2. *LightGBM + Optuna*\n",
    "3. *CatBoost + Optuna*\n",
    "4. *Histogram-Based Gradient Boosting (HistGBM) + Optuna*\n",
    "5. *Extra Trees + Optuna*\n",
    "6. *Random Forest + Optuna*\n",
    "7. *AdaBoost + Optuna*\n",
    "8. *Blending*\n",
    "9. *Stacking*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough Structure for (Model + Optuna) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "print(f\"Code started at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train=\"training data\"\n",
    "test=\"testing data\"\n",
    "\n",
    "# Splitting df_train into features (X) and target (y)\n",
    "x = train.drop(columns=['targets'])\n",
    "y = train['targets']\n",
    "\n",
    "# Splitting train and test datasets\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Hyperparameter sampling\n",
    "    # 2. Model training\n",
    "    # 3. Performance evaluation\n",
    "    # 4. Return the score\n",
    "    rf_model = sklearn.ensemble.RandomForestRegressor(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
    "        max_depth=trial.suggest_int('max_depth', 10, 30, log=True),\n",
    "        min_samples_split=trial.suggest_int('min_samples_split', 2, 10),\n",
    "        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "        max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.5]),\n",
    "        bootstrap=trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        random_state=42\n",
    "    )\n",
    "    cv_score = sklearn.model_selection.cross_val_score(rf_model, x_train, y_train, \n",
    "                               cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return -cv_score  # Minimize negative MSE\n",
    "\n",
    "# Optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50,timeout=600)\n",
    "\n",
    "# Train the best model\n",
    "best_rf_model = sklearn.ensemble.RandomForestRegressor(**study.best_params, random_state=42)\n",
    "best_rf_model.fit(x_train, y_train,)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = best_rf_model.predict(x_test)\n",
    "print(f\"Predictions on X_test: {predictions[:10]}\")\n",
    "\n",
    "#submission\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "output = pd.DataFrame({\"id\":sub.id, \"Premium Amount\":predictions})\n",
    "output.to_csv('submission_final.csv', index=False)\n",
    "\n",
    "\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all trial results into a pandas DataFrame for further analysis\n",
    "print(\"\\nAll Trials as DataFrame:\")\n",
    "df = study.trials_dataframe()\n",
    "print(df.head())  # Display the first few rows of the DataFrame\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Print the best objective value (negative MSE in this case)\n",
    "print(\"\\nBest Objective Value (Negative MSE):\")\n",
    "print(study.best_value)\n",
    "\n",
    "# Print details of the best trial\n",
    "print(\"\\nDetails of the Best Trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"  Trial Number: {best_trial.number}\")\n",
    "print(f\"  Best Params: {best_trial.params}\")\n",
    "print(f\"  Best Value (Negative MSE): {best_trial.value}\")\n",
    "\n",
    "# Print user-defined and system attributes of the best trial\n",
    "print(\"\\nAdditional Attributes of the Best Trial:\")\n",
    "print(f\"  User Attributes: {best_trial.user_attrs}\")\n",
    "print(f\"  System Attributes: {best_trial.system_attrs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model's + Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\admin\\onedrive\\desktop\\machine learning\\virtaul_env\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\admin\\onedrive\\desktop\\machine learning\\virtaul_env\\lib\\site-packages (from lightgbm) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\onedrive\\desktop\\machine learning\\virtaul_env\\lib\\site-packages (from lightgbm) (1.14.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "\n",
    "\n",
    "gbm = lgbm.train(param, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "# Initialize a RandomForestRegressor model with default parameters\n",
    "randomforest_model = sklearn.ensemble.RandomForestRegressor(\n",
    "rf_hyperparam_grid = {\n",
    "    'n_estimators': [50, 100, 200],                    # Number of trees to evaluate.\n",
    "    'max_depth': [None, 10, 20, 30],                  # Different depths to test tree complexity.\n",
    "    'min_samples_split': [2, 5, 10],                 # Adjusts minimum samples required to split.\n",
    "    'min_samples_leaf': [1, 2, 4],                   # Tests minimum samples per leaf.\n",
    "    'max_features': ['sqrt', 'log2', 0.5],           # Explore feature subsets for splitting.\n",
    "    'bootstrap': [True, False],                      # Tests bootstrap and non-bootstrap methods.\n",
    "    'oob_score': [True, False],                      # Includes out-of-bag scoring in evaluation.\n",
    "    'random_state': [42]                             # Ensures reproducibility during tuning.\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extratree_model = sklearn.tree.ExtraTreeRegressor(\n",
    "{\n",
    "    \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"],\n",
    "    \"splitter\": [\"random\", \"best\"],\n",
    "    \"max_depth\": [None, 10, 20, 50, 100],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"max_features\": [None, \"auto\", \"sqrt\", \"log2\", 0.5, 1.0],\n",
    "    \"min_impurity_decrease\": [0.0, 0.01, 0.1],\n",
    "    \"max_leaf_nodes\": [None, 10, 50, 100],\n",
    "    \"ccp_alpha\": [0.0, 0.01, 0.1, 1.0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_model = sklearn.ensemble.AdaBoostRegressor(\n",
    "{\n",
    "    \"n_estimators\": [50, 100, 200, 500],\n",
    "    \"learning_rate\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"loss\": [\"linear\", \"square\", \"exponential\"],\n",
    "    \"random_state\": [None, 0, 42, 100]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model = sklearn.ensemble.GradientBoostingRegressor(\n",
    "    \"loss\": [\"squared_error\", \"absolute_error\", \"huber\", \"quantile\"],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2, 0.5],\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [3, 5, 10, None],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "        tree_method='gpu_hist',  # GPU acceleration for faster training\n",
    "        lambda_=trial.suggest_loguniform('lambda', 1e-3, 10.0),  # Regularization term\n",
    "        alpha=trial.suggest_loguniform('alpha', 1e-3, 10.0),  # L1 regularization\n",
    "        colsample_bytree=trial.suggest_categorical('colsample_bytree', [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),  # Fraction of features used\n",
    "        subsample=trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 1.0]),  # Fraction of data used\n",
    "        learning_rate=trial.suggest_categorical('learning_rate', [0.008, 0.01, 0.012, 0.014, 0.016, 0.018, 0.02]),  # Learning rate\n",
    "        n_estimators=10000,  # Number of trees (max iterations)\n",
    "        max_depth=trial.suggest_categorical('max_depth', [5, 7, 9, 11, 13, 15, 17]),  # Depth of trees\n",
    "        random_state=trial.suggest_categorical('random_state', [2020]),  # Random seed\n",
    "        min_child_weight=trial.suggest_int('min_child_weight', 1, 300)  # Minimum weight of child\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "xgboost_model = sklearn.ensemble.HistGradientBoostingRegressor(\n",
    "{\n",
    "    \"loss\": [\"squared_error\", \"absolute_error\", \"poisson\", \"quantile\"],  # Loss function options\n",
    "    \"quantile\": [None, 0.1, 0.2, 0.3, 0.4, 0.5],  # Quantile for \"quantile\" loss, only applicable for quantile loss\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],  # Learning rate options\n",
    "    \"max_iter\": [50, 100, 200],  # Number of boosting iterations\n",
    "    \"max_leaf_nodes\": [31, 50, 100, None],  # Max leaf nodes per tree, None means unlimited\n",
    "    \"max_depth\": [None, 5, 10, 20],  # Max depth of each tree\n",
    "    \"min_samples_leaf\": [5, 10, 20, 50],  # Minimum samples per leaf node\n",
    "    \"l2_regularization\": [0.0, 0.1, 0.5, 1.0],  # Regularization strength to prevent overfitting\n",
    "    \"early_stopping\": [False, \"auto\"],  # Enable early stopping\n",
    "    \"scoring\": [\"loss\", \"neg_mean_squared_error\", \"r2\"],  # Scoring metric for early stopping\n",
    "}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lightgbm\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost \n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "\n",
    "model = CatBoostRegressor(\n",
    "    loss_function='RMSE',\n",
    "    task_type='GPU',\n",
    "    l2_leaf_reg=trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
    "    max_bin=trial.suggest_int('max_bin', 200, 400),\n",
    "    subsample=trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "    learning_rate=trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
    "    n_estimators=25000,\n",
    "    max_depth=trial.suggest_categorical('max_depth', [5,7,9,11,13,15]),\n",
    "    random_state=trial.suggest_categorical('random_state', [2020]),\n",
    "    min_data_in_leaf=trial.suggest_int('min_data_in_leaf', 1, 300)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LightGBM + Optuna\n",
    "\n",
    "CatBoost + Optuna\n",
    "\n",
    "Stacking\n",
    "\n",
    "Blending\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtaul_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
