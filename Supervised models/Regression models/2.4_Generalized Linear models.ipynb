{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized linear models :\n",
    "1. GammaRegressor\n",
    "2. PoissonRegressor\n",
    "3. TweedieRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GammaRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition:\n",
    "\"\"\"\n",
    "GammaRegressor:\n",
    "A generalized linear model for regression that models the relationship between the target variable \n",
    "and features using a gamma distribution. It is useful for modeling data that is strictly positive and \n",
    "skewed, such as data in finance, healthcare, or engineering.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.linear_model import GammaRegressor\n",
    "\n",
    "# Initialize GammaRegressor with default or custom parameters\n",
    "gammaregressor_model = GammaRegressor(\n",
    "    alpha=1.0,                  # Constant that multiplies the L2 penalty term for regularization (default=1)\n",
    "    fit_intercept=True,         # Whether to add an intercept term to the model (default=True)\n",
    "    solver=\"lbfgs\",             # Optimization algorithm to solve the regression problem (default='lbfgs')\n",
    "    max_iter=100,               # Maximum number of iterations for optimization (default=100)\n",
    "    tol=1e-4,                   # Stopping criterion for the optimization algorithm (default=1e-4)\n",
    "    warm_start=False,           # Whether to reuse the solution of the previous fit (default=False)\n",
    "    verbose=0                   # Verbosity level for the solver, set to a positive number for verbosity (default=0)\n",
    ")\n",
    "\n",
    "# Hyperparameters:\n",
    "gammaregressor_hyperparameters = {\n",
    "    \"alpha\": [0.0, 0.1, 1.0, 10.0],                # Regularization strength (L2 penalty term) (default=1)\n",
    "    \"fit_intercept\": [True, False],                 # Whether to fit the intercept term (default=True)\n",
    "    \"solver\": ['lbfgs', 'newton-cholesky'],        # Solver to use for optimization (default='lbfgs')\n",
    "    \"max_iter\": [100, 500, 1000, 5000],            # Maximum number of iterations for the solver (default=100)\n",
    "    \"tol\": [1e-5, 1e-4, 1e-3, 1e-2],               # Tolerance for stopping criterion (default=1e-4)\n",
    "    \"warm_start\": [True, False],                   # Whether to reuse previous solution for initialization (default=False)\n",
    "    \"verbose\": [0, 1, 2]                           # Verbosity level for the solver (default=0)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoissonRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition:\n",
    "\"\"\"\n",
    "PoissonRegressor:\n",
    "A generalized linear model for regression that assumes the target variable follows a Poisson distribution.\n",
    "It is useful for modeling count data or rates that are non-negative integers, such as event counts over time.\n",
    "\"\"\"\n",
    "\n",
    "# Code Example:\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "# Initialize PoissonRegressor with default or custom parameters\n",
    "poissonregressor_model = PoissonRegressor(\n",
    "    alpha=1,                     # Regularization strength (L2 penalty term) (default=1)\n",
    "    fit_intercept=True,          # Whether to fit the intercept term (default=True)\n",
    "    solver=\"lbfgs\",              # Solver to use for optimization (default='lbfgs')\n",
    "    max_iter=100,                # Maximum number of iterations for the solver (default=100)\n",
    "    tol=1e-4,                    # Stopping criterion for the optimization algorithm (default=1e-4)\n",
    "    warm_start=False,            # Whether to reuse the solution of the previous fit (default=False)\n",
    "    verbose=0                    # Verbosity level for the solver (default=0)\n",
    ")\n",
    "\n",
    "# Hyperparameters:\n",
    "poissonregressor_hyperparameters = {\n",
    "    \"alpha\": [0.0, 0.1, 1.0, 10.0],                # Regularization strength (L2 penalty term) (default=1)\n",
    "    \"fit_intercept\": [True, False],                 # Whether to fit the intercept term (default=True)\n",
    "    \"solver\": ['lbfgs', 'newton-cholesky'],        # Solver to use for optimization (default='lbfgs')\n",
    "    \"max_iter\": [100, 500, 1000, 5000],            # Maximum number of iterations for the solver (default=100)\n",
    "    \"tol\": [1e-5, 1e-4, 1e-3, 1e-2],               # Tolerance for stopping criterion (default=1e-4)\n",
    "    \"warm_start\": [True, False],                   # Whether to reuse previous solution for initialization (default=False)\n",
    "    \"verbose\": [0, 1, 2]                           # Verbosity level for the solver (default=0)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweedieRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition:\n",
    "\"\"\"\n",
    "TweedieRegressor:\n",
    "A Generalized Linear Model (GLM) that uses the Tweedie distribution, which can model a range of distributions \n",
    "depending on the power parameter. It is particularly useful when modeling data that follows distributions \n",
    "such as Poisson, Gamma, or Normal, and can handle different types of regression problems based on the power parameter.\n",
    "\"\"\"\n",
    "\n",
    "# Code Example:\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "# Initialize TweedieRegressor with default or custom parameters\n",
    "tweedie_regressor_model = TweedieRegressor(\n",
    "    power=0,                    # Power parameter to control the distribution (default=0 for Normal distribution)\n",
    "    alpha=1,                     # Regularization strength (L2 penalty term) (default=1)\n",
    "    fit_intercept=True,          # Whether to fit the intercept term (default=True)\n",
    "    link=\"auto\",                 # Link function, 'auto', 'identity', or 'log' (default='auto')\n",
    "    solver=\"lbfgs\",              # Solver to use for optimization (default='lbfgs')\n",
    "    max_iter=100,                # Maximum number of iterations for the solver (default=100)\n",
    "    tol=1e-4,                    # Stopping criterion for the optimization algorithm (default=1e-4)\n",
    "    warm_start=False,            # Whether to reuse the solution of the previous fit (default=False)\n",
    "    verbose=0                    # Verbosity level for the solver (default=0)\n",
    ")\n",
    "\n",
    "# Hyperparameters:\n",
    "tweedie_regressor_hyperparameters = {\n",
    "    \"power\": [0.0, 1.0, 1.5, 2.0, 3.0],               # Power parameter controls the underlying distribution (default=0)\n",
    "    \"alpha\": [0.0, 0.1, 1.0, 10.0],                   # Regularization strength (L2 penalty term) (default=1)\n",
    "    \"fit_intercept\": [True, False],                    # Whether to fit the intercept term (default=True)\n",
    "    \"link\": ['auto', 'identity', 'log'],               # Link function for GLM (default='auto')\n",
    "    \"solver\": ['lbfgs', 'newton-cholesky'],           # Solver to use for optimization (default='lbfgs')\n",
    "    \"max_iter\": [100, 500, 1000, 5000],               # Maximum number of iterations for the solver (default=100)\n",
    "    \"tol\": [1e-5, 1e-4, 1e-3, 1e-2],                  # Tolerance for stopping criterion (default=1e-4)\n",
    "    \"warm_start\": [True, False],                       # Whether to reuse previous solution (default=False)\n",
    "    \"verbose\": [0, 1, 2]                               # Verbosity level for the solver (default=0)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition:\n",
    "\"\"\"\n",
    "PassiveAggressiveRegressor:\n",
    "A regression model that uses a passive-aggressive algorithm, ideal for large-scale regression problems\n",
    "where the data may have noisy or outlying observations. It is particularly effective in online learning\n",
    "settings where data arrives sequentially.\n",
    "\"\"\"\n",
    "\n",
    "# Code Example:\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "\n",
    "# Initialize PassiveAggressiveRegressor with default or custom parameters\n",
    "passiveaggressiveregressor_model = PassiveAggressiveRegressor(\n",
    "    C=1.0,                      # Regularization strength (default=1.0)\n",
    "    fit_intercept=True,         # Whether to fit the intercept term (default=True)\n",
    "    max_iter=1000,              # Maximum number of iterations (default=1000)\n",
    "    tol=1e-3,                   # Stopping criterion (default=1e-3)\n",
    "    early_stopping=False,       # Whether to use early stopping (default=False)\n",
    "    validation_fraction=0.1,    # Fraction of training data to use for validation when early_stopping=True (default=0.1)\n",
    "    n_iter_no_change=5,         # Number of iterations with no improvement to wait before early stopping (default=5)\n",
    "    shuffle=True,               # Whether to shuffle data after each epoch (default=True)\n",
    "    verbose=0,                  # Verbosity level for the solver (default=0)\n",
    "    loss=\"epsilon_insensitive\", # Loss function (default=\"epsilon_insensitive\")\n",
    "    epsilon=0.1,                # Epsilon value for \"epsilon_insensitive\" loss (default=0.1)\n",
    "    random_state=None,          # Random state for reproducibility (default=None)\n",
    "    warm_start=False,           # Whether to reuse the solution of the previous call (default=False)\n",
    "    average=False               # Whether to compute averaged SGD weights (default=False)\n",
    ")\n",
    "\n",
    "# Hyperparameters:\n",
    "passiveaggressiveregressor_hyperparameters = {\n",
    "    \"C\": [0.1, 1.0, 10.0, 100.0],               # Regularization strength (default=1.0)\n",
    "    \"fit_intercept\": [True, False],              # Whether to fit the intercept term (default=True)\n",
    "    \"max_iter\": [1000, 5000, 10000],            # Maximum number of iterations (default=1000)\n",
    "    \"tol\": [1e-5, 1e-4, 1e-3, 1e-2],            # Stopping criterion (default=1e-3)\n",
    "    \"early_stopping\": [True, False],            # Whether to use early stopping (default=False)\n",
    "    \"validation_fraction\": [0.05, 0.1, 0.2],    # Fraction of training data for validation when early_stopping=True (default=0.1)\n",
    "    \"n_iter_no_change\": [5, 10, 20],            # Number of iterations with no improvement to wait (default=5)\n",
    "    \"shuffle\": [True, False],                   # Whether to shuffle data after each epoch (default=True)\n",
    "    \"verbose\": [0, 1, 2],                       # Verbosity level (default=0)\n",
    "    \"loss\": [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"], # Loss function (default=\"epsilon_insensitive\")\n",
    "    \"epsilon\": [0.05, 0.1, 0.2],                # Epsilon value for \"epsilon_insensitive\" loss (default=0.1)\n",
    "    \"random_state\": [None, 42, 0],              # Random state for reproducibility (default=None)\n",
    "    \"warm_start\": [True, False],                # Whether to reuse the previous solution (default=False)\n",
    "    \"average\": [True, False]                    # Whether to compute averaged SGD weights (default=False)\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtaul_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
