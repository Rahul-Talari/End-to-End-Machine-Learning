{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Kernel Transformation vs. Kernel Approximation:</u>\n",
    "- Kernel transformations map data into a higher-dimensional feature space, making non-linear relationships linearly separable. However, this is computationally expensive for large datasets.\n",
    "\n",
    "- Kernel approximations approximate this high-dimensional mapping with lower-dimensional feature spaces. While this reduces computational cost, it retains the essential characteristics of the original kernel, allowing linear methods to perform efficiently.\n",
    "\n",
    "\n",
    "<u>Kernel Approximations Explained:</u>\n",
    "\n",
    "- Kernel approximations help scale kernel methods efficiently for large datasets.\n",
    "- Traditional kernel methods, like the Radial Basis Function (RBF) kernel, compute a kernel matrix that grows quadratically with the dataset size, making them computationally expensive and memory-intensive.\n",
    "\n",
    "- Kernel approximations, such as Random Fourier Features for RBF kernels or the Nystroem method, transform input data into a lower-dimensional space that mimics the original kernelâ€™s behavior. This enables the use of faster, linear algorithms while still capturing non-linear relationships.\n",
    "\n",
    "\n",
    "<u>Example: Concentric Circles Problem</u>\n",
    "- Consider a dataset where points form concentric circles, which are not linearly separable in 2D. An RBF kernel maps these points into a higher-dimensional space, where they can be separated by a hyperplane. However, this transformation is costly.\n",
    "\n",
    "- Random Fourier Features approximate this mapping by projecting the data into a lower-dimensional space, preserving the separability characteristics. \n",
    "This enables efficient linear methods like SVM while achieving similar performance to the original kernel method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Kernel Approximation Methods and Their Data Suitability</h4>\n",
    "\n",
    "<table style=\"font-size: 14px;\">\n",
    "  <tr>\n",
    "    <th><strong>Method</strong></th>\n",
    "    <th><strong>Description</strong></th>\n",
    "    <th><strong>Data Type It Handles Well</strong></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>AdditiveChi2Sampler</strong></td>\n",
    "    <td>Approximates the feature map for the additive chi-squared kernel. It is ideal for non-negative data such as histograms or counts.</td>\n",
    "    <td><strong>Non-negative data, Histograms, Count data</strong></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Nystroem</strong></td>\n",
    "    <td>Uses a subset of the training data to approximate the kernel map. Ideal for large datasets, it is computationally efficient.</td>\n",
    "    <td><strong>Large datasets, Data with high dimensionality</strong></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>PolynomialCountSketch</strong></td>\n",
    "    <td>Approximates polynomial kernels via the Tensor Sketch algorithm. Efficient for polynomial kernel-based learning.</td>\n",
    "    <td><strong>Data requiring polynomial kernel, High-dimensional spaces</strong></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>RBFSampler</strong></td>\n",
    "    <td>Approximates the RBF kernel feature map by using random Fourier features, making it suitable for large-scale datasets.</td>\n",
    "    <td><strong>High-dimensional, continuous data, Data requiring smooth decision boundaries (such as in clustering, regression, or classification tasks with continuous features)</strong></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>SkewedChi2Sampler</strong></td>\n",
    "    <td>Approximates the feature map for \"skewed chi-squared\" kernel, often used for data with skewed distributions.</td>\n",
    "    <td><strong>Non-negative, skewed data (e.g., counts, frequencies)</strong></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdditiveChi2Square approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Random Feature Selection  : Randomly selects a subset of features.\n",
    "- Random Sampling           : Generates random points (basis functions) for transformation.\n",
    "- Transformation & Modeling : Transforms data using Chi-squared kernel approximation and applies a linear model.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.kernel_approximation\n",
    "\n",
    "# Initialize the AdditiveChi2Sampler\n",
    "additivechi2sampler = sklearn.kernel_approximation.AdditiveChi2Sampler(\n",
    "    sample_steps=2,                #  Defines how many random points are sampled for kernel approximation. Default is 2.\n",
    "    sample_interval=None           #  Specifies the spacing between these sampled points. Default is None.\n",
    ")\n",
    "\n",
    "\n",
    "# Example of using the AdditiveChi2Sampler\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import AdditiveChi2Sampler\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# Apply AdditiveChi2Sampler transformation\n",
    "chi2sampler = AdditiveChi2Sampler(sample_steps=2)\n",
    "X_transformed = chi2sampler.fit_transform(X, y)\n",
    "\n",
    "# Train classifier\n",
    "clf = SGDClassifier(max_iter=5, random_state=0, tol=1e-3)\n",
    "clf.fit(X_transformed, y)\n",
    "\n",
    "# Evaluate the classifier\n",
    "clf_score = clf.score(X_transformed, y)\n",
    "print(f\"Classifier score: {clf_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nystroem approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Kernel Approximation : The Nystroem method approximates the feature map of a kernel to speed up computations.\n",
    "- Used with Linear Models : Works well with linear models such as LinearSVC for efficient non-linear classification.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.kernel_approximation\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "# Initialize the Nystroem approximation\n",
    "nystroem_approximation = sklearn.kernel_approximation.Nystroem(\n",
    "    kernel=\"rbf\",                   # Specifies the kernel to be approximated. Can be 'rbf', 'polynomial', etc.\n",
    "    gamma=0.2,                      # Gamma parameter for the RBF kernel. Controls the width of the Gaussian. Default is None.\n",
    "                                    # Higher values lead to a narrower kernel, affecting the decision boundary.\n",
    "    coef0=None,                     # Coefficient for polynomial and sigmoid kernels (not used for RBF). Default is None.\n",
    "    degree=None,                    # Degree for polynomial kernel (not used for RBF). Default is None.\n",
    "    kernel_params=None,             # Additional parameters for custom kernels passed as a callable function. Default is None.\n",
    "    n_components=300,               # Number of features to construct during kernel approximation. More components increase the computational complexity.\n",
    "                                    # Larger values may lead to better accuracy at the cost of speed and memory.\n",
    "    random_state=None,              # Controls randomness for reproducibility. None means random behavior; set an integer (e.g., 0, 42) for consistent results.\n",
    "    n_jobs=-1,                      # Number of CPU cores to use for computation (-1 uses all available cores). Default is 1.\n",
    ")\n",
    "param_grid = {\n",
    "    'kernel': ['rbf', 'polynomial', 'laplacian', 'sigmoid', 'chi2'],  # Type of kernel function to approximate\n",
    "    'gamma': [0.01, 0.1, 0.5, 1, 2, None],  # Gamma parameter for the kernel function (None is the default)\n",
    "    'coef0': [None, 0.0, 0.1, 0.5, 1.0],  # Coefficient for polynomial and sigmoid kernels (ignored for others)\n",
    "    'degree': [2, 3, 4, 5, 6],  # Degree for polynomial kernel (ignored for other kernels)\n",
    "    'n_components': [50, 100, 200, 300, 400],  # Number of components for feature mapping\n",
    "    'random_state': [None, 0, 42],  # Random state for reproducibility\n",
    "    'n_jobs': [None, 1, -1]  # Number of CPU cores to use for computation, None means 1 core, -1 uses all available cores\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset (digits)\n",
    "X, y = datasets.load_digits(n_class=9, return_X_y=True)\n",
    "\n",
    "# Normalize the data (scale the input features)\n",
    "data = X / 16.  # Normalize by dividing by 16, as the original data is in range [0, 16]\n",
    "\n",
    "# Apply Nystroem kernel approximation\n",
    "data_transformed = nystroem_approximation.fit_transform(data)\n",
    "\n",
    "# Train a Linear Support Vector Classifier on the transformed data\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(data_transformed, y)\n",
    "\n",
    "# Evaluate the classifier\n",
    "clf_score = clf.score(data_transformed, y)\n",
    "print(f\"Classifier score: {clf_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PolynomialCountSketch Approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Polynomial Kernel Approximation : Approximates the feature map of the polynomial kernel using Tensor Sketch.\n",
    "- Uses Fast Fourier Transforms (FFT) to efficiently compute a Count Sketch of the outer product of a vector with itself.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.kernel_approximation\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Initialize PolynomialCountSketch with specified hyperparameters\n",
    "polynomialcountSketch_approximation = sklearn.kernel_approximation.PolynomialCountSketch(\n",
    "    gamma=1.0,             # Parameter of the polynomial kernel (K(X, Y) = (gamma * <X, Y> + coef0)^degree)\n",
    "    degree=3,              # Degree of the polynomial kernel (default is 2, here set to 3)\n",
    "    coef0=1,               # Constant term (default is 0)\n",
    "    n_components=100,      # Dimensionality of the output feature space (usually greater than number of input features)\n",
    "    random_state=None,     # Controls randomness for reproducibility. None means random behavior; set an integer (e.g., 0, 42) for consistent results.\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.kernel_approximation import PolynomialCountSketch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "ps = PolynomialCountSketch(degree=3, random_state=1)\n",
    "X_features = ps.fit_transform(X)\n",
    "\n",
    "clf = SGDClassifier(max_iter=10, tol=1e-3)\n",
    "\n",
    "clf.fit(X_features, y)\n",
    "SGDClassifier(max_iter=10)\n",
    "clf.score(X_features, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBFSampler Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Random Fourier Features: Approximates the RBF kernel by generating random Fourier features.\n",
    "- Feature Transformation: Uses RBF kernel approximation to project the input data into a higher-dimensional space.\n",
    "- Linear Classification: Applies a linear model after kernel approximation for classification or regression tasks.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.kernel_approximation\n",
    "\n",
    "# Initialize the RBFSampler for kernel approximation\n",
    "rbfsampler_approximation = sklearn.kernel_approximation.RBFSampler(\n",
    "    gamma=1,                # - Controls the width of the RBF kernel, influencing how localized the influence of each data point is.\n",
    "                            # - Higher gamma makes the kernel more sensitive, leading to a complex model.\n",
    "                            # - Lower gamma makes the kernel broader, leading to a smoother model.\n",
    "                            # - gamma = 'scale' automatically adjusts based on data variance and feature count.Controls the width of the kernel. Default is 1.\n",
    "    \n",
    "    n_components=100,       # - Defines the number of random features to use in approximating the RBF kernel.\n",
    "                            # - Higher n_components increases accuracy but also computation cost and dimensionality.\n",
    "                            # - Lower n_components reduces accuracy but speeds up the process; Default is 100.    \n",
    "    random_state=None,      # Controls randomness for reproducibility. None means random behavior; set an integer (e.g., 0, 42) for consistent results.\n",
    "\n",
    ")\n",
    "\n",
    "# Example of using the RBFSampler to transform data and apply a classifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Load the dataset\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# Apply RBFSampler transformation to the data (kernel approximation)\n",
    "rbf_feature = sklearn.kernel_approximation.RBFSampler(gamma=1, random_state=1)\n",
    "X_features = rbf_feature.fit_transform(X)  # Transform data using RBF kernel approximation\n",
    "\n",
    "# Train a classifier using the transformed features\n",
    "clf = SGDClassifier(max_iter=5, random_state=0, tol=1e-3)  # Initialize SGD classifier\n",
    "clf.fit(X_features, y)  # Fit the classifier to the transformed data\n",
    "\n",
    "# Evaluate the classifier on the transformed data\n",
    "clf_score = clf.score(X_features, y)  # Calculate the accuracy score\n",
    "print(f\"Classifier score: {clf_score:.4f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkewedChi2sample Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- SkewedChi2Sampler: Approximates the chi-square kernel with skewed features for kernel approximation.\n",
    "- Feature Transformation: Uses SkewedChi2Sampler to project the input data into a higher-dimensional space.\n",
    "- Linear Classification: Applies a linear model after kernel approximation for classification tasks.\n",
    "\"\"\"\n",
    "import sklearn.kernel_approximation\n",
    "\n",
    "skewedchi2sample_approximation = sklearn.kernel_approximation.SkewedChi2Sampler(\n",
    "    skewedness=0.01,           # - Controls the skewness of the chi-square transformation. \n",
    "                               # - Lower values make the transformation more uniform, higher values make it more skewed.\n",
    "    n_components=10,           # - Number of random features to use in approximating the chi-square kernel.\n",
    "                               # - Increasing n_components increases accuracy but also computational cost.\n",
    "    random_state=0             # - Controls randomness for reproducibility. Set an integer for consistent results.\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.kernel_approximation import SkewedChi2Sampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "# Example of using the SkewedChi2Sampler to transform data and apply a classifier\n",
    "X = [[0, 0], [1, 1], [1, 0], [0, 1]]  # Input data\n",
    "y = [0, 0, 1, 1]                       # Target labels\n",
    "\n",
    "# Apply SkewedChi2Sampler transformation to the data (kernel approximation)\n",
    "X_features = skewedchi2sample_approximation.fit_transform(X, y)\n",
    "\n",
    "# Train a classifier using the transformed features\n",
    "clf = SGDClassifier(max_iter=10, tol=1e-3)  # Initialize SGD classifier with 10 iterations\n",
    "clf.fit(X_features, y)  # Fit the classifier to the transformed data\n",
    "\n",
    "# Evaluate the classifier on the transformed data\n",
    "clf_score = clf.score(X_features, y)  # Calculate the accuracy score\n",
    "print(f\"Classifier score: {clf_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtaul_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
