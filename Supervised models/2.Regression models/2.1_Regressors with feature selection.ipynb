{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressors with Variable selection:\n",
    "\n",
    "1. Lars, LarsCV, Lars_path\n",
    "2. Lasso, LassoCV, Lasso_path\n",
    "3. LassoLars, LassoLarsCV\n",
    "4. ElasticNet, ElasticNetCV, Enet_path\n",
    "5. OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lars Regression (Least Angle Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import numpy as np\n",
    "\n",
    "lars_model=sklearn.linear_model.Lars(\n",
    "    fit_intercept=True,          # Whether to include an intercept in the model (default is True). If False, the model is fitted without an intercept (data is assumed to be centered).\n",
    "    verbose=False,                           # Verbosity level. If set to `1`, the solver will print basic progress information (e.g., iteration count). Higher values give more detailed output; default=False\n",
    "    precompute='auto',                       # Whether to precompute the Gram matrix for speed. Options: 'auto', True, False, or array-like.\n",
    "    n_nonzero_coefs=500,                      # Controls sparsity: smaller values lead to sparser models, np.inf allows unlimited non-zero coefficients; default=500 \n",
    "    eps=np.finfo(float).eps,                 # Regularization parameter for numerical stability. \n",
    "                                             # It represents the smallest positive number such that 1.0 + eps != 1.0.\n",
    "                                             # This helps prevent numerical errors in matrix factorization, particularly in the Cholesky decomposition.\n",
    "                                             # If the system is ill-conditioned (e.g., very close to singular), you can increase `eps` \n",
    "                                             # to add a small amount of regularization and improve stability.\n",
    "                                             # By default, it's set to the smallest value that can be represented by the machine's floating-point type.\n",
    "    random_state=None,                        # Controls the randomness for reproducibility. None means random behavior, or set an integer (e.g., 0, 42) for consistent results.\n",
    "\n",
    "\n",
    "    copy_X=True,                             # Whether to copy the input data (True) or modify it in-place (False). Default is True.\n",
    "    fit_path=True,                           # If True, stores the coefficient path; False speeds up computation, especially for large datasets.\n",
    "    jitter=None,                             # Optional noise added to target `y` for stability. Example values: [None, 1e-4, 1e-3].\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter dictionary for tuning\n",
    "lars_hyperparameters = {\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"verbose\": [0, 1, 2],\n",
    "    \"precompute\": ['auto', True, False],\n",
    "    \"n_nonzero_coefs\": [50, 100, 500, np.inf], \n",
    "    \"eps\": [1e-8, 1e-5, np.finfo(float).eps],  \n",
    "    \"copy_X\": [True, False],\n",
    "    \"fit_path\": [True, False],\n",
    "    \"jitter\": [None, 1e-4, 1e-3],              \n",
    "    \"random_state\": [None, 0, 42]              \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "larscv_model=sklearn.linear_model.LarsCV(\n",
    "    fit_intercept=True,                       # Whether to include an intercept in the model (default is True). If False, the model is fitted without an intercept (data is assumed to be centered).\n",
    "    verbose=False,                           # Verbosity level. If set to `1`, the solver will print basic progress information (e.g., iteration count). Higher values give more detailed output; default=False\n",
    "    precompute='auto',                       # Whether to precompute the Gram matrix for speed. Options: 'auto', True, False, or array-like.\n",
    "\n",
    "    cv=5,                                    # Default: 5-fold cross-validation. Use integer (e.g., cv=10) or custom splitters for specific splitting strategies.\n",
    "    max_n_alphas=1000,                       # Default value is 1000\n",
    "                                             # max_n_alphas specifies the maximum number of alpha values to test during cross-validation.\n",
    "                                             # Each alpha controls the regularization strength (how much the coefficients are penalized).\n",
    "                                             # Example:\n",
    "                                                # - A smaller value (e.g., max_n_alphas=100) speeds up training but explores fewer options.\n",
    "                                                # - A larger value (e.g., max_n_alphas=5000) tests more alphas but increases computation time.\n",
    "    n_jobs=-1,                               # Number of CPU cores to use for computation (-1 uses all available cores). Default is 1.\n",
    "    eps=np.finfo(float).eps                  # Default: Smallest positive float. Adds regularization for numerical stability, especially in ill-conditioned systems.copy_X=True,                             # Whether to copy the input data (True) or modify it in-place (False). Default is True.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition:\n",
    "\"\"\"\n",
    "lars_path:\n",
    "The Least Angle Regression (LARS) algorithm computes the path of coefficients for a linear model\n",
    "as a function of the regularization parameter. It's an efficient method for performing regression \n",
    "with L1 regularization, especially suited for sparse data.\n",
    "\"\"\"\n",
    "\n",
    "# Code Example:\n",
    "from sklearn.linear_model import lars_path\n",
    "\n",
    "# Call lars_path with the required parameters\n",
    "lars_path_model = lars_path(\n",
    "    X=None,                        # Input data matrix (features) - required input\n",
    "    y=None,                        # Target data array - required input\n",
    "    Xy=None,                       # Pre-computed Gram matrix or dot(X.T, y) (optional)\n",
    "    Gram=None,                     # Precomputed Gram matrix or 'auto' (default=None)\n",
    "    max_iter=500,                  # Maximum number of iterations (default=500)\n",
    "    alpha_min=0.0,                 # Minimum value of the regularization parameter (default=0)\n",
    "    method=\"lar\",                  # Method for path computation, 'lar' for LARS, 'lasso' for Lasso (default=\"lar\")\n",
    "    copy_X=True,                   # Whether to copy X (default=True)\n",
    "    eps=1e-5,                      # Convergence tolerance (default=1e-5)\n",
    "    copy_Gram=True,                # Whether to copy the Gram matrix (default=True)\n",
    "    verbose=0,                     # Verbosity level for the solver (default=0)\n",
    "    return_path=True,              # Whether to return the full path (default=True)\n",
    "    return_n_iter=False,           # Whether to return the number of iterations (default=False)\n",
    "    positive=False                 # Whether to enforce positivity of the coefficients (default=False)\n",
    ")\n",
    "\n",
    "# Hyperparameters:\n",
    "lars_path_hyperparameters = {\n",
    "    \"max_iter\": [100, 500, 1000, 5000],              # Maximum number of iterations (default=500)\n",
    "    \"alpha_min\": [0.0, 0.1, 0.5, 1.0],               # Minimum value of the regularization parameter (default=0)\n",
    "    \"method\": [\"lar\", \"lasso\"],                       # Method for path computation ('lar' or 'lasso')\n",
    "    \"copy_X\": [True, False],                          # Whether to copy the input matrix (default=True)\n",
    "    \"eps\": [1e-5, 1e-4, 1e-3, 1e-2],                 # Convergence tolerance (default=1e-5)\n",
    "    \"copy_Gram\": [True, False],                       # Whether to copy the Gram matrix (default=True)\n",
    "    \"verbose\": [0, 1, 2],                             # Verbosity level (default=0)\n",
    "    \"return_path\": [True, False],                     # Whether to return the full path of coefficients (default=True)\n",
    "    \"return_n_iter\": [True, False],                   # Whether to return the number of iterations (default=False)\n",
    "    \"positive\": [True, False]                         # Whether to enforce positivity of coefficients (default=False)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import Lasso  # type: ignore\n",
    "\n",
    "lasso_model = Lasso(\n",
    "    alpha=1.0,                # Regularization strength. Larger values mean stronger regularization (reducing overfitting); smaller values mean less regularization. Default is 1.0.\n",
    "    fit_intercept=True,          # Whether to include an intercept in the model (default is True). If False, the model is fitted without an intercept (data is assumed to be centered).\n",
    "    precompute=False,         # Whether to precompute the Gram matrix for speed. If True, computations are faster for small datasets.Default is False.\n",
    "    max_iter=1000,            # Maximum number of iterations for optimization. Higher values allow the solver more time to converge.\n",
    "    tol=0.0001,               # Precision for convergence. Lower values mean higher precision.\n",
    "    warm_start=False,         # Whether to reuse the solution of the previous fit as the initialization for the next one. Useful for iterative fitting.\n",
    "    positive=False,           # If True, restricts coefficients to be positive. Default is False.\n",
    "    random_state=None,        # Controls randomness for reproducibility. None means random behavior; set an integer (e.g., 0, 42) for consistent results.\n",
    "    selection=\"cyclic\",       # Strategy for coefficient updates. \"cyclic\" updates one at a time; \"random\" selects coefficients randomly.\n",
    ")\n",
    "\n",
    "lasso_params = {\n",
    "    \"alpha\": [0.1, 1.0, 10.0],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"max_iter\": [1000, 5000, 10000],\n",
    "    \"tol\": [0.0001, 0.001, 0.01],\n",
    "    \"warm_start\": [True, False],\n",
    "    \"positive\": [True, False],\n",
    "    \"random_state\": [None, 42, 0],\n",
    "    \"selection\": [\"cyclic\", \"random\"]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Key Notes:\n",
    "- Regularization: Helps prevent overfitting by penalizing large coefficients.\n",
    "- Selection: \"Cyclic\" processes coefficients in order, while \"random\" adds randomness, which may converge faster in some cases.\n",
    "- Random State: Ensures consistent results for randomness-dependent behavior, useful for debugging.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "lassocv_model = sklearn.linear_model.LassoCV(\n",
    "    eps=0.001,                   # Length of the path. eps=1e-3 means alpha_min / alpha_max = 1e-3. Default is 0.001.\n",
    "    n_alphas=100,                # Number of alpha values along the regularization path. Default is 100.\n",
    "    alphas=(0.1, 1.0, 10.0),     # Array of alpha values. If None, alphas are set automatically. Default is None.\n",
    "    fit_intercept=True,          # Whether to include an intercept in the model (default is True). If False, the model is fitted without an intercept (data is assumed to be centered).\n",
    "    precompute=False,            # Whether to precompute the Gram matrix for speed. Default is False.\n",
    "    max_iter=1000,               # Maximum number of iterations for optimization. Default is 1000.\n",
    "    tol=0.0001,                  # Precision for convergence. Default is 0.0001.\n",
    "    copy_X=True,                 # Whether to copy the input data or modify it in-place. Default is True.\n",
    "\n",
    "    cv=5,                        # Cross-validation strategy. Default is 5-fold CV.\n",
    "                                 # Options:\n",
    "                                 # - int: Number of folds (e.g., 5 for 5-fold CV).\n",
    "                                 # - BaseCrossValidator: Custom strategies like KFold or StratifiedKFold.\n",
    "                                 # - BaseShuffleSplit: Strategies like ShuffleSplit.\n",
    "                                 # - Iterable: Custom train-test split indices.\n",
    "                                 # - None: Defaults to 5-fold cross-validation.\n",
    "\n",
    "\n",
    "    verbose=False,               # Verbosity level. If set to `1`, the solver will print basic progress information (e.g., iteration count). Higher values give more detailed output; default=False\n",
    "    n_jobs=-1,                   # Number of CPU cores to use for computation (-1 uses all available cores). Default is 1.\n",
    "    positive=False,              # If True, restricts coefficients to be positive. Default is False.\n",
    "    random_state=None,           # Controls randomness for reproducibility. Default is None.\n",
    "    selection=\"cyclic\"           # Strategy for coefficient updates. Default is \"cyclic\".\n",
    "                                 # Options:\n",
    "                                 # - \"cyclic\": Updates coefficients one at a time.\n",
    "                                 # - \"random\": Selects coefficients randomly for updates.\n",
    ")\n",
    "\n",
    "lassocv_hyperparameters = {\n",
    "    \"eps\": [0.001, 0.01, 0.1],              \n",
    "    \"n_alphas\": [50, 100, 200],             \n",
    "    \"alphas\": [(0.01, 0.1, 1.0), None],     \n",
    "    \"fit_intercept\": [True, False],         \n",
    "    \"precompute\": [True, False],            \n",
    "    \"max_iter\": [1000, 5000, 10000],        \n",
    "    \"tol\": [0.0001, 0.001, 0.01],           \n",
    "    \"copy_X\": [True, False],                \n",
    "    \"cv\": [3, 5, 10, None],                 \n",
    "    \"verbose\": [False, True],               \n",
    "    \"n_jobs\": [1, -1],                      \n",
    "    \"positive\": [True, False],              \n",
    "    \"random_state\": [None, 42, 0],          \n",
    "    \"selection\": [\"cyclic\", \"random\"],      \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition:\n",
    "\"\"\"\n",
    "lasso_path:\n",
    "Lasso path algorithm computes the path of coefficients for a Lasso model across a range of alpha values. \n",
    "It is useful for performing L1-regularized linear regression, especially when there is sparse data.\n",
    "\"\"\"\n",
    "\n",
    "# Code Example:\n",
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "# Call lasso_path with the required parameters\n",
    "lasso_path_model = lasso_path(\n",
    "    X=None,                        # Input data matrix (features) - required input\n",
    "    y=None,                        # Target data array - required input\n",
    "    eps=0.001,                     # Convergence tolerance (default=0.001)\n",
    "    n_alphas=100,                  # Number of alpha values to use in the path (default=100)\n",
    "    alphas=None,                   # List or array of alpha values to compute the path (default=None)\n",
    "    precompute=\"auto\",             # Whether to precompute the Gram matrix (default='auto')\n",
    "    Xy=None,                       # Pre-computed Gram matrix or dot(X.T, y) (optional)\n",
    "    copy_X=True,                   # Whether to copy X (default=True)\n",
    "    coef_init=None,                # Initial coefficients for warm start (default=None)\n",
    "    verbose=False,                 # Verbosity level (default=False)\n",
    "    return_n_iter=False,           # Whether to return the number of iterations (default=False)\n",
    "    positive=False                 # Whether to enforce positivity of coefficients (default=False)\n",
    ")\n",
    "\n",
    "# Hyperparameters:\n",
    "lasso_path_hyperparameters = {\n",
    "    \"eps\": [1e-5, 1e-4, 0.001, 0.01],         # Convergence tolerance (default=0.001)\n",
    "    \"n_alphas\": [50, 100, 200, 500],          # Number of alphas to use in the path (default=100)\n",
    "    \"alphas\": [None, [0.1, 0.5, 1.0, 5.0]],   # List of alpha values to compute (default=None)\n",
    "    \"precompute\": [True, False, \"auto\"],      # Whether to precompute the Gram matrix (default=\"auto\")\n",
    "    \"Xy\": [None],                            # Pre-computed Gram matrix or dot(X.T, y) (default=None)\n",
    "    \"copy_X\": [True, False],                 # Whether to copy X (default=True)\n",
    "    \"coef_init\": [None],                     # Initial coefficients for warm start (default=None)\n",
    "    \"verbose\": [0, 1, 2],                    # Verbosity level (default=False)\n",
    "    \"return_n_iter\": [True, False],          # Whether to return the number of iterations (default=False)\n",
    "    \"positive\": [True, False]                # Whether to enforce positivity of coefficients (default=False)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoLars Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "lassolars_model = sklearn.linear_model.LassoLars(\n",
    "    alpha=1.0,                   # Regularization strength. Larger values mean stronger regularization. Default is 1.0.\n",
    "    fit_intercept=True,          # Whether to fit the intercept. If False, the data is assumed to be centered. Default is True.\n",
    "    verbose=False,               # Verbosity level. If set to `1`, the solver will print basic progress information (e.g., iteration count). Higher values give more detailed output; default=False\n",
    "    \n",
    "    precompute=\"auto\",           # Whether to use a precomputed Gram matrix. \"auto\" lets the model decide. Default is \"auto\".\n",
    "    max_iter=500,                # Maximum number of iterations to run. Default is 500.\n",
    "    eps=np.finfo(float).eps,     # Precision of the solution. Default is machine epsilon for stability.\n",
    "    copy_X=True,                 # Whether to copy the input data or modify it in-place. Default is True.\n",
    "    fit_path=True,               # If True, computes the full path for the coefficients along the regularization path. Default is True.\n",
    "    positive=False,              # If True, restricts coefficients to be positive. Default is False.\n",
    "    jitter=None,                 # Adds small random noise to the input data for stability. Default is None (no jitter).\n",
    "    random_state=None            # Controls randomness for reproducibility. Default is None (random behavior).\n",
    ")\n",
    "\n",
    "lassolars_hyperparameters = {\n",
    "    \"alpha\": [0.1, 1.0, 10.0],                     \n",
    "    \"fit_intercept\": [True, False],                \n",
    "    \"verbose\": [False, True],                      \n",
    "    \"normalize\": [True, False, \"deprecated\"],      \n",
    "    \"precompute\": [\"auto\", True, False],           \n",
    "    \"max_iter\": [500, 1000, 2000],                 \n",
    "    \"eps\": [1e-15, 1e-6, 1e-3],                    \n",
    "    \"copy_X\": [True, False],                       \n",
    "    \"fit_path\": [True, False],                     \n",
    "    \"positive\": [True, False],                     \n",
    "    \"jitter\": [None, 1e-5, 1e-3],                  \n",
    "    \"random_state\": [None, 42, 0],                 \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Key Notes:\n",
    "- Regularization: Helps control overfitting by penalizing large coefficients.\n",
    "- Fit Path: When enabled, provides the full regularization path, which can be useful for analysis but slower.\n",
    "- Jitter: Adding small noise can stabilize the solution, particularly for ill-conditioned problems.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "lassolarscv_model = sklearn.linear_model.LassoLarsCV(\n",
    "    fit_intercept=True,                       # Whether to include an intercept in the model (default is True). If False, the model is fitted without an intercept (data is assumed to be centered).\n",
    "    verbose=False,                           # Verbosity level. If set to `1`, the solver will print basic progress information (e.g., iteration count). Higher values give more detailed output; default=False\n",
    "    max_iter=500,                            # Maximum number of iterations for optimization. Default is 500.\n",
    "    precompute='auto',                       # Whether to precompute the Gram matrix for speed. Options: 'auto', True, False, or array-like.\n",
    "    cv=5,                                    # Cross-validation strategy. Default is 5-fold CV.\n",
    "    max_n_alphas=1000,                       # Maximum number of alpha values for path optimization. Default is 1000.\n",
    "    n_jobs=None,                             # Number of CPU cores to use for computation. Default is None (use 1 core).\n",
    "    eps=np.finfo(float).eps,                 # Precision for solution stability. Default is machine epsilon.\n",
    "    copy_X=True,                             # Whether to copy the input data or modify it in-place. Default is True.\n",
    "    positive=False                           # If True, restricts coefficients to be positive. Default is False.\n",
    ")\n",
    "\n",
    "lassolarscv_hyperparameters = {\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"verbose\": [False, True],\n",
    "    \"max_iter\": [500, 1000, 2000],\n",
    "    \"precompute\": [\"auto\", True, False],\n",
    "    \"cv\": [3, 5, 10, None],\n",
    "    \"max_n_alphas\": [500, 1000, 2000],\n",
    "    \"n_jobs\": [None, 1, -1],\n",
    "    \"eps\": [1e-15, 1e-6, 1e-3],\n",
    "    \"copy_X\": [True, False],\n",
    "    \"positive\": [True, False],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "#(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * l1_ratio * ||w||_1 + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
    "\n",
    "elasticnet_model = sklearn.linear_model.ElasticNet(\n",
    "    alpha=1,                        # Regularization strength, controls the magnitude of coefficients.\n",
    "    l1_ratio=0.5,                   # The mix ratio between Lasso (L1) and Ridge (L2) regularization.\n",
    "                                    # 0.5 means equal weight to both Lasso and Ridge.\n",
    "    fit_intercept=True,             # Whether to include an intercept in the model (default is True). If False, the model is fitted without an intercept (data is assumed to be centered).\n",
    "    precompute=False,               # Whether to use a precomputed Gram matrix to speed up computations.\n",
    "    max_iter=1000,                  # Maximum number of iterations for optimization.\n",
    "    copy_X=True,                    # Whether to modify the input data or copy it for processing.\n",
    "    tol=0.0001,                     # Precision for convergence. Defines the stopping criteria for optimization.\n",
    "    warm_start=False,               # Whether to reuse previous solutions to warm start the next model.\n",
    "    positive=False,                 # If True, restricts coefficients to be positive.\n",
    "    random_state=None,              # Controls randomness for reproducibility. Defaults to None.\n",
    "    selection=\"cyclic\"              # Strategy for coefficient updates. \"cyclic\" updates coefficients one at a time.\n",
    ")\n",
    "\n",
    "# Defining the hyperparameter grid for ElasticNet\n",
    "elasticnet_hyperparameters = {\n",
    "    \"alpha\": [0.1, 0.5, 1, 10],   \n",
    "    \"l1_ratio\": [0.1, 0.5, 0.7, 1.0],  \n",
    "    \"fit_intercept\": [True, False],    \n",
    "    \"precompute\": [True, False],       \n",
    "    \"max_iter\": [1000, 5000, 10000],   \n",
    "    \"copy_X\": [True, False],           \n",
    "    \"tol\": [1e-5, 1e-4, 0.0001],       \n",
    "    \"warm_start\": [True, False],       \n",
    "    \"positive\": [True, False],         \n",
    "    \"random_state\": [None, 42, 0],     \n",
    "    \"selection\": [\"cyclic\", \"random\"]  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "elasticnetcv_model = sklearn.linear_model.ElasticNetCV(\n",
    "    l1_ratio=0.5,               # The mixing parameter for L1 (Lasso) and L2 (Ridge) regularization. Default is 0.5.\n",
    "    eps=0.001,                  # Smallest alpha value as a fraction of alpha_max. Default is 0.001.\n",
    "    n_alphas=100,               # Number of alpha values along the regularization path. Default is 100.\n",
    "    alphas=None,                # List of alpha values. If None, values are set automatically. Default is None.\n",
    "    fit_intercept=True,         # Whether to include an intercept in the model. Default is True.\n",
    "    precompute=\"auto\",          # Whether to precompute the Gram matrix. Default is 'auto'.\n",
    "    max_iter=1000,              # Maximum iterations for convergence. Default is 1000.\n",
    "    tol=0.0001,                 # Tolerance for convergence. Default is 0.0001.\n",
    "    cv=5,                       # Cross-validation strategy. Default is 5-fold CV.\n",
    "                                # Options:\n",
    "                                # - int: Number of folds (e.g., 5 for 5-fold CV).\n",
    "                                # - BaseCrossValidator: Custom strategies like KFold or StratifiedKFold.\n",
    "                                # - BaseShuffleSplit: Strategies like ShuffleSplit.\n",
    "                                # - Iterable: Custom train-test split indices.\n",
    "                                # - None: Defaults to 5-fold cross-validation.\n",
    "    copy_X=True,                # Whether to copy the input data or modify it in-place. Default is True.\n",
    "    verbose=0,                  # Verbosity level for output. Default is 0 (silent).\n",
    "    n_jobs=-1,                  # Number of CPU cores to use (-1 uses all available cores). Default is None.\n",
    "    positive=False,             # Restricts coefficients to be positive if True. Default is False.\n",
    "    random_state=None,          # Sets random seed for reproducibility. Default is None.\n",
    "    selection=\"cyclic\"          # Coefficient update strategy. Default is 'cyclic'.\n",
    "                                # Options:\n",
    "                                # - \"cyclic\": Updates coefficients sequentially.\n",
    "                                # - \"random\": Selects coefficients randomly for updates.\n",
    ")\n",
    "\n",
    "elasticnetcv_hyperparameters = {\n",
    "    \"l1_ratio\": [0.1, 0.5, 0.9],\n",
    "    \"eps\": [0.001, 0.01, 0.1],\n",
    "    \"n_alphas\": [50, 100, 200],\n",
    "    \"alphas\": [(0.01, 0.1, 1.0), None],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"precompute\": [True, False, \"auto\"],\n",
    "    \"max_iter\": [1000, 5000, 10000],\n",
    "    \"tol\": [0.0001, 0.001, 0.01],\n",
    "    \"copy_X\": [True, False],\n",
    "    \"cv\": [3, 5, 10, None],\n",
    "    \"verbose\": [0, 1],\n",
    "    \"n_jobs\": [1, -1],\n",
    "    \"positive\": [True, False],\n",
    "    \"random_state\": [None, 42, 0],\n",
    "    \"selection\": [\"cyclic\", \"random\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition:\n",
    "\"\"\"\n",
    "enet_path:\n",
    "ElasticNet path algorithm computes the path of coefficients for a linear regression model \n",
    "using ElasticNet regularization across a range of alpha values. It combines L1 (Lasso) and L2 (Ridge) penalties.\n",
    "The optimization is done using coordinate descent. It is useful for situations where the features have \n",
    "correlation and we want to include both regularizations.\n",
    "\"\"\"\n",
    "\n",
    "# Code Example:\n",
    "from sklearn.linear_model import enet_path\n",
    "\n",
    "# Call enet_path with the required parameters\n",
    "enetpath_model = enet_path(\n",
    "    X=None,                        # Input data matrix (features) - required input\n",
    "    y=None,                        # Target data array - required input\n",
    "    l1_ratio=0.5,                  # The mix ratio between L1 and L2 penalties (default=0.5)\n",
    "    eps=0.001,                     # Convergence tolerance (default=0.001)\n",
    "    n_alphas=100,                  # Number of alpha values to use in the path (default=100)\n",
    "    alphas=None,                   # List or array of alpha values to compute the path (default=None)\n",
    "    precompute=\"auto\",             # Whether to precompute the Gram matrix (default='auto')\n",
    "    Xy=None,                       # Pre-computed Gram matrix or dot(X.T, y) (optional)\n",
    "    copy_X=True,                   # Whether to copy X (default=True)\n",
    "    coef_init=None,                # Initial coefficients for warm start (default=None)\n",
    "    verbose=False,                 # Verbosity level (default=False)\n",
    "    return_n_iter=False,           # Whether to return the number of iterations (default=False)\n",
    "    positive=False,                # Whether to enforce positivity of coefficients (default=False)\n",
    "    check_input=True,              # Whether to check the input validity (default=True)\n",
    ")\n",
    "\n",
    "# Hyperparameters:\n",
    "enetpath_hyperparameters = {\n",
    "    \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 1.0],     # The mix ratio between L1 and L2 penalties (default=0.5)\n",
    "    \"eps\": [1e-5, 1e-4, 0.001, 0.01],          # Convergence tolerance (default=0.001)\n",
    "    \"n_alphas\": [50, 100, 200, 500],           # Number of alphas to use in the path (default=100)\n",
    "    \"alphas\": [None, [0.1, 0.5, 1.0, 5.0]],    # List of alpha values to compute (default=None)\n",
    "    \"precompute\": [True, False, \"auto\"],       # Whether to precompute the Gram matrix (default=\"auto\")\n",
    "    \"Xy\": [None],                             # Pre-computed Gram matrix or dot(X.T, y) (default=None)\n",
    "    \"copy_X\": [True, False],                  # Whether to copy X (default=True)\n",
    "    \"coef_init\": [None],                      # Initial coefficients for warm start (default=None)\n",
    "    \"verbose\": [0, 1, 2],                     # Verbosity level (default=False)\n",
    "    \"return_n_iter\": [True, False],           # Whether to return the number of iterations (default=False)\n",
    "    \"positive\": [True, False],                # Whether to enforce positivity of coefficients (default=False)\n",
    "    \"check_input\": [True, False]              # Whether to check the input validity (default=True)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OrthogonalMatchingPursuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "orthogonalmatchingpursuit_model = sklearn.linear_model.OrthogonalMatchingPursuit(\n",
    "    n_nonzero_coefs=None,        # Desired number of non-zero entries in the solution. Default is None.\n",
    "                                 # Ignored if tol is set. If both are None, defaults to 10% of n_features or 1, whichever is greater.\n",
    "    tol=None,                    # Maximum squared norm of the residual. Default is None.\n",
    "                                 # Overrides n_nonzero_coefs if specified.\n",
    "    fit_intercept=True,          # Whether to include an intercept in the model (default is True). If False, the model is fitted without an intercept (data is assumed to be centered).\n",
    "\n",
    "    precompute=\"auto\"              # Whether to use a precomputed Gram and Xy matrix to speed up calculations. Default is 'auto'.\n",
    "                                 # Helps with large n_targets or n_samples. Use False to disable, or pass precomputed matrices.\n",
    ")\n",
    "\n",
    "orthogonalmatchingpursuit_hyperparameters = {\n",
    "    \"n_nonzero_coefs\": [None, 10, 20],\n",
    "    \"tol\": [None, 0.01, 0.001],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"normalize\": [True, False, \"deprecated\"],\n",
    "    \"precompute\": [True, False, \"auto\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "orthogonalmatchingpursuitcv_model = sklearn.linear_model.OrthogonalMatchingPursuitCV(\n",
    "    copy=True,                  # Whether the design matrix X must be copied by the algorithm. Default is True.\n",
    "    fit_intercept=True,         # Whether to calculate the intercept for this model. Default is True. If False, data is assumed to be centered.\n",
    "    max_iter=None,              # Maximum number of iterations to perform. Default is None. 10% of n_features but at least 5 if available.\n",
    "    cv=5,                       # Cross-validation strategy. Default is 5-fold CV.\n",
    "    n_jobs=-1,                  # Number of CPU cores to use (-1 uses all available cores). Default is None.\n",
    "    verbose=0,                  # Verbosity level for output. Default is 0 (silent).\n",
    ")\n",
    "\n",
    "orthogonalmatchingpursuitcv_hyperparameters = {\n",
    "    \"copy\": [True, False],             \n",
    "    \"fit_intercept\": [True, False],        \n",
    "    \"max_iter\": [None, 1000, 5000],        \n",
    "    \"cv\": [3, 5, 10, None],                \n",
    "    \"n_jobs\": [1, -1],                    \n",
    "    \"verbose\": [0, 1, 2],                 \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtaul_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
