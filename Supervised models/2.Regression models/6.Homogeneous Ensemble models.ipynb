{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging:\n",
    "- Bagging Regressor\n",
    "- Random Forest\n",
    "- Extra trees(Extremely Randomized Trees)\n",
    "- Isolation Forest\n",
    "\n",
    "Boosting:\n",
    "- Adaboost\n",
    "- Gradient Boostinhg Machine\n",
    "- Histogram GBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "bagging_regressor = sklearn.ensemble.BaggingRegressor(\n",
    "    n_estimators=10,                    # Number of base estimators in the ensemble\n",
    "    max_samples=1.0,                     # The fraction of samples to train each base estimator on (can be an integer for absolute count)\n",
    "    max_features=1.0,                    # The fraction of features to train each base estimator on (can be an integer for absolute count)\n",
    "    bootstrap=True,                      # Whether samples are drawn with replacement\n",
    "    bootstrap_features=False,            # Whether features are drawn with replacement\n",
    "    oob_score=False,                     # Whether to use out-of-bag samples to estimate generalization accuracy\n",
    "    warm_start=False,                    # Whether to reuse previous modelâ€™s results to fit new estimators\n",
    "    n_jobs=None,                         # Number of jobs to run in parallel (None means 1)\n",
    "    random_state=None,                   # Seed used by the random number generator (for reproducibility)\n",
    "    verbose=0                            # Verbosity level. 0 means silent, higher values give more information\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 200],         \n",
    "    \"max_samples\": [0.5, 1.0, 0.8, 0.6],        \n",
    "    \"max_features\": [0.5, 1.0, 0.8, 0.6],       \n",
    "    \"bootstrap\": [True, False],                  \n",
    "    \"bootstrap_features\": [True, False],         \n",
    "    \"oob_score\": [True, False],                \n",
    "    \"warm_start\": [True, False],                \n",
    "    \"n_jobs\": [None, 1, 2, 4],                  \n",
    "    \"random_state\": [None, 42, 123],            \n",
    "    \"verbose\": [0, 1, 2]                        \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "# Initialize a RandomForestRegressor model with default parameters\n",
    "randomforest_model = sklearn.ensemble.RandomForestRegressor(\n",
    "    n_estimators=100,                      # Number of trees in the forest; higher values may improve accuracy but increase computation time.\n",
    "    criterion=\"squared_error\",            # Loss function to measure split quality; 'squared_error' for regression tasks.\n",
    "    max_depth=None,                        # Maximum depth of the tree; smaller values prevent overfitting.\n",
    "    min_samples_split=2,                   # Minimum samples required to split an internal node; larger values increase model generalization.\n",
    "    min_samples_leaf=1,                    # Minimum samples required in a leaf node; larger values smooth the model.\n",
    "   \n",
    "    min_weight_fraction_leaf=0,            # Minimum weighted fraction of the sum of weights for leaf nodes; used for imbalanced data.\n",
    "    max_features=1,                        # Number of features considered for the best split; 1 for all features, 'sqrt' or 'log2' for subsets.\n",
    "    max_leaf_nodes=None,                   # Maximum number of leaf nodes; limits complexity and avoids overfitting.\n",
    "    min_impurity_decrease=0,               # Minimum impurity decrease required for a split; larger values create simpler trees.\n",
    "    bootstrap=True,                        # Whether to use bootstrap samples; True enables bagging, improving robustness.\n",
    "    oob_score=False,                       # Whether to use out-of-bag samples to estimate generalization error; useful for validation.\n",
    "    n_jobs=None,                           # Number of jobs to run in parallel; -1 uses all processors.\n",
    "    random_state=None,                     # Seed for random number generation; ensures reproducibility.\n",
    "    verbose=0,                             # Controls verbosity when fitting and predicting; higher values print more info.\n",
    "    warm_start=False,                      # Reuse solution of the previous call to fit; useful for adding estimators incrementally.\n",
    "    ccp_alpha=0,                           # Complexity parameter for Minimal Cost-Complexity Pruning; larger values simplify the model.\n",
    "    max_samples=None                       # Number or fraction of samples to draw for each base estimator; limits training data.\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning dictionary\n",
    "rf_hyperparam_grid = {\n",
    "    'n_estimators': [50, 100, 200],                    # Number of trees to evaluate.\n",
    "    'max_depth': [None, 10, 20, 30],                  # Different depths to test tree complexity.\n",
    "    'min_samples_split': [2, 5, 10],                 # Adjusts minimum samples required to split.\n",
    "    'min_samples_leaf': [1, 2, 4],                   # Tests minimum samples per leaf.\n",
    "    'max_features': ['sqrt', 'log2', 0.5],           # Explore feature subsets for splitting.\n",
    "    'bootstrap': [True, False],                      # Tests bootstrap and non-bootstrap methods.\n",
    "    'oob_score': [True, False],                      # Includes out-of-bag scoring in evaluation.\n",
    "    'random_state': [42]                             # Ensures reproducibility during tuning.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "isolationforest_model = IsolationForest(\n",
    "    n_estimators=100,              # Number of trees in the forest\n",
    "    max_samples=\"auto\",            # Number of samples to draw for each tree ('auto' uses all samples)\n",
    "    contamination=\"auto\",          # Proportion of outliers (or 'auto' for automatic estimation)\n",
    "    max_features=1.0,              # Proportion of features to consider at each split\n",
    "    bootstrap=False,               # Whether to use bootstrap samples\n",
    "    n_jobs=None,                   # Number of parallel jobs (-1 uses all processors)\n",
    "    random_state=None,             # Seed for reproducibility\n",
    "    verbose=0,                     # Verbosity level of output\n",
    "    warm_start=False               # If True, reuse trees from previous fit\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of trees; Small values (50) reduce computation but may lower accuracy; Large values (150) improve model robustness at the cost of computation.\n",
    "\n",
    "    'max_samples': [0.5, 0.75, 'auto'],  # Fraction of samples per tree; Small values (0.5) increase randomness but may lower stability; Larger values (0.75) provide more data per tree, improving stability.\n",
    "\n",
    "    'contamination': [0.05, 0.1, 0.2],  # Proportion of anomalies in data; Small values (0.05) assume fewer anomalies, possibly missing some; Larger values (0.2) increase anomaly detection but might misclassify normal points.\n",
    "\n",
    "    'max_features': [0.5, 0.75, 1.0],  # Fraction of features to use at each split; Small values (0.5) add randomness and reduce overfitting but may miss important features; Larger values (1.0) improve accuracy but may risk overfitting.\n",
    "\n",
    "    'bootstrap': [False, True],  # Whether to use bootstrapping; False uses unique subsets for each tree, increasing diversity; True uses overlapping subsets, which can improve performance on small datasets.\n",
    "\n",
    "    'random_state': [42]  # Random seed for reproducibility; A fixed value (42) ensures consistent results across runs, useful for debugging and comparison.\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=IsolationForest(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',  # Use an appropriate metric for your task\n",
    "    cv=3,                # 3-fold cross-validation\n",
    "    n_jobs=-1,           # Use all processors\n",
    "    verbose=1            # Show progress\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "extratree_model = sklearn.tree.ExtraTreeRegressor(\n",
    "    criterion=\"squared_error\",              # The function to measure the quality of a split. Options: 'squared_error' (default), 'friedman_mse', 'absolute_error', 'poisson'.\n",
    "    splitter=\"random\",                      # Strategy used to split at each node. 'random' (default) for random splits, 'best' for best split.\n",
    "    max_depth=None,                         # The maximum depth of the tree. None means the tree expands until all leaves are pure or min_samples_split is reached.\n",
    "    min_samples_split=2,                    # Minimum number of samples required to split a node. Small values make the tree more flexible.\n",
    "    min_samples_leaf=1,                     # Minimum number of samples required to be a leaf node. Larger values prevent overfitting.\n",
    "   \n",
    "    min_weight_fraction_leaf=0,             # Minimum weighted fraction of input samples required to be at a leaf node. Useful for balancing class distribution.\n",
    "    max_features=1,                         # Number of features to consider for splits. 'auto', 'sqrt', 'log2' or specific float/int values. Higher values make splits more flexible.\n",
    "    random_state=None,                      # Controls randomness of the estimator. Set to an integer for reproducibility.\n",
    "    min_impurity_decrease=0,                # A node will split if the impurity decrease is at least this value. Prevents splits with negligible gain.\n",
    "    max_leaf_nodes=None,                    # Limits the number of leaf nodes. None means unlimited leaf nodes.\n",
    "    ccp_alpha=0                             # Complexity parameter for Minimal Cost-Complexity Pruning. Larger values prune more aggressively.\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"],\n",
    "    \"splitter\": [\"random\", \"best\"],\n",
    "    \"max_depth\": [None, 10, 20, 50, 100],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"max_features\": [None, \"auto\", \"sqrt\", \"log2\", 0.5, 1.0],\n",
    "    \"min_impurity_decrease\": [0.0, 0.01, 0.1],\n",
    "    \"max_leaf_nodes\": [None, 10, 50, 100],\n",
    "    \"ccp_alpha\": [0.0, 0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=extratree_model,               # The ExtraTreeRegressor model\n",
    "    param_grid=param_grid,   # The hyperparameter grid\n",
    "    cv=5,                                   # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',       # Scoring metric\n",
    "    verbose=1,                              # Verbosity level for logging\n",
    "    n_jobs=-1                               # Use all available processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "adaboost_model = sklearn.ensemble.AdaBoostRegressor(\n",
    "    n_estimators=50,                 # Number of weak learners (estimators). Default is 50. Increasing this can improve performance but may lead to overfitting.\n",
    "    learning_rate=1.0,               # Weight applied to each regressor at each iteration. Lower values slow down learning.\n",
    "    loss=\"linear\",                   # Loss function to minimize. Options: 'linear' (default), 'square', 'exponential'.\n",
    "    random_state=None                # Random seed for reproducibility. Set an integer for deterministic results.\n",
    ")\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200, 500],\n",
    "    \"learning_rate\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"loss\": [\"linear\", \"square\", \"exponential\"],\n",
    "    \"random_state\": [None, 0, 42, 100]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=adaboost_model, \n",
    "    param_grid=param_grid, \n",
    "    scoring=\"neg_mean_squared_error\",  # Evaluation metric\n",
    "    cv=5,                             # 5-fold cross-validation\n",
    "    verbose=1,                        # Verbosity level\n",
    "    n_jobs=-1                         # Use all available processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "gbm_model = sklearn.ensemble.GradientBoostingRegressor(\n",
    "    loss=\"squared_error\",              # Loss function to optimize. Options: 'squared_error', 'absolute_error', 'huber', 'quantile'.\n",
    "    learning_rate=0.1,                 # Learning rate shrinks the contribution of each tree. Trade-off with n_estimators.\n",
    "    n_estimators=100,                  # Number of boosting stages to fit. Higher values can improve performance but increase training time.\n",
    "    subsample=1.0,                     # Fraction of samples to use for fitting each tree. Lower values add randomness for robustness.\n",
    "    max_depth=3,                       # Maximum depth of individual trees. Limits tree size to control overfitting.\n",
    "    \n",
    "    criterion=\"friedman_mse\",          # Criterion for measuring the quality of a split. Options: 'friedman_mse', 'squared_error'.\n",
    "    min_samples_split=2,               # Minimum number of samples required to split a node.\n",
    "    min_samples_leaf=1,                # Minimum number of samples required to be at a leaf node.\n",
    "    min_weight_fraction_leaf=0.0,      # Minimum weighted fraction of samples required to be at a leaf node.\n",
    "    min_impurity_decrease=0.0,         # Minimum decrease in impurity for a split to be made.\n",
    "    init=None,                         # Initial estimator for boosting. 'zero' or a prefit model can be used.\n",
    "    random_state=None,                 # Controls randomness for reproducibility.\n",
    "    max_features=None,                 # Number of features to consider when looking for the best split.\n",
    "    alpha=0.9,                         # Quantile for the huber and quantile loss functions.\n",
    "    verbose=0,                         # Verbosity level for logging. 0 is silent, higher values show more details.\n",
    "    max_leaf_nodes=None,               # Limits the number of leaf nodes. Helps control overfitting.\n",
    "    warm_start=False,                  # If True, reuse solution from previous fit calls to add more estimators.\n",
    "    validation_fraction=0.1,           # Proportion of training data used for early stopping validation.\n",
    "    n_iter_no_change=None,             # Number of iterations with no improvement to stop training early.\n",
    "    tol=0.0001,                        # Tolerance for stopping criterion.\n",
    "    ccp_alpha=0.0                      # Complexity parameter for minimal cost-complexity pruning.\n",
    ")\n",
    "param_grid = {\n",
    "    \"loss\": [\"squared_error\", \"absolute_error\", \"huber\", \"quantile\"],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2, 0.5],\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [3, 5, 10, None],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "\n",
    "    \"max_features\": [None, \"auto\", \"sqrt\", \"log2\"],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    \"min_impurity_decrease\": [0.0, 0.01, 0.1],\n",
    "    \"ccp_alpha\": [0.0, 0.01, 0.1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hist GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "xgboost_model = sklearn.ensemble.HistGradientBoostingRegressor(\n",
    "    loss=\"squared_error\",               # Loss function. Options: \"squared_error\", \"absolute_error\", \"poisson\", \"quantile\".\n",
    "    quantile=None,                      # Quantile to optimize when loss=\"quantile\". Ignored for other loss functions.\n",
    "    learning_rate=0.1,                  # Shrinks the contribution of each tree. Lower values require more iterations.\n",
    "    max_iter=100,                       # Number of boosting iterations.\n",
    "    max_leaf_nodes=31,                  # Max leaf nodes per tree. None means unlimited nodes.\n",
    "    max_depth=None,                     # Max depth of each tree. None means no limit.\n",
    "    min_samples_leaf=20,                # Minimum number of samples required in a leaf node to reduce overfitting.\n",
    "    l2_regularization=0.0,              # Strength of L2 regularization to prevent overfitting.\n",
    "    scoring=\"loss\",                     # Metric used for early stopping. Options: \"loss\", or a custom scorer.\n",
    "    \n",
    "    \n",
    "    max_bins=255,                       # Maximum number of bins used for feature quantization.\n",
    "    categorical_features=None,          # Indicates which features are categorical. None means all are treated as numerical.\n",
    "    monotonic_cst=None,                 # Monotonic constraints for each feature.\n",
    "    interaction_cst=None,               # Constraints on feature interactions. \n",
    "    warm_start=False,                   # Reuse the solution of the previous fit for the next call to fit.\n",
    "    early_stopping=\"auto\",              # Enable early stopping to terminate training when validation score does not improve.\n",
    "    validation_fraction=0.1,            # Proportion of training data used as validation data for early stopping.\n",
    "    n_iter_no_change=10,                # Number of iterations with no improvement to trigger early stopping.\n",
    "    tol=1e-7,                           # Tolerance for the stopping condition.\n",
    "    verbose=0,                          # Verbosity level. 0 is silent.\n",
    "    random_state=None                   # Random seed for reproducibility.\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"loss\": [\"squared_error\", \"absolute_error\", \"poisson\", \"quantile\"],  # Loss function options\n",
    "    \"quantile\": [None, 0.1, 0.2, 0.3, 0.4, 0.5],  # Quantile for \"quantile\" loss, only applicable for quantile loss\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],  # Learning rate options\n",
    "    \"max_iter\": [50, 100, 200],  # Number of boosting iterations\n",
    "    \"max_leaf_nodes\": [31, 50, 100, None],  # Max leaf nodes per tree, None means unlimited\n",
    "    \"max_depth\": [None, 5, 10, 20],  # Max depth of each tree\n",
    "    \"min_samples_leaf\": [5, 10, 20, 50],  # Minimum samples per leaf node\n",
    "    \"l2_regularization\": [0.0, 0.1, 0.5, 1.0],  # Regularization strength to prevent overfitting\n",
    "    \"early_stopping\": [False, \"auto\"],  # Enable early stopping\n",
    "    \"scoring\": [\"loss\", \"neg_mean_squared_error\", \"r2\"],  # Scoring metric for early stopping\n",
    "\n",
    "    \n",
    "    \"max_bins\": [128, 255, 512],  # Maximum number of bins for quantization\n",
    "    \"categorical_features\": [None, [0, 1], [0, 2]],  # Categorical feature indices\n",
    "    \"monotonic_cst\": [None, {0: 1}, {1: -1}],  # Monotonic constraints (None or a dictionary of constraints)\n",
    "    \"interaction_cst\": [None, \"pairwise\", [(0, 1), (1, 2)]],  # Interaction constraints (None, pairwise or custom constraints)\n",
    "    \"warm_start\": [False, True],  # Whether to reuse previous solution\n",
    "    \"early_stopping\": [False, \"auto\"],  # Enable early stopping\n",
    "    \"validation_fraction\": [0.05, 0.1, 0.2],  # Fraction of training data used for validation\n",
    "    \"n_iter_no_change\": [5, 10, 20],  # Number of iterations with no improvement for early stopping\n",
    "    \"tol\": [1e-7, 1e-6, 1e-5],  # Tolerance for stopping condition\n",
    "    \"verbose\": [0, 1, 2],  # Verbosity level\n",
    "    \"random_state\": [None, 42, 123]  # Random seed for reproducibility\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtaul_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
