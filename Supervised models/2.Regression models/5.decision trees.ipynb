{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree # type: ignore\n",
    "\n",
    "# Create a DecisionTreeRegressor model with default values explicitly assigned\n",
    "decisiontree_model = sklearn.tree.DecisionTreeRegressor(\n",
    "    criterion=\"squared_error\",           # Default: \"squared_error\". Measures the quality of a split.\n",
    "                                         # - \"squared_error\": Minimizes mean squared error.\n",
    "                                         # - \"friedman_mse\": Optimized for boosting.\n",
    "                                         # - \"absolute_error\": Minimizes mean absolute error.\n",
    "                                         # - \"poisson\": Suitable for count data.\n",
    "    \n",
    "    splitter=\"best\",                     # Default: \"best\". Strategy for splitting at each node:\n",
    "                                         # - \"best\": Chooses the best split based on the criterion.\n",
    "                                         # - \"random\": Chooses a random split at each node.\n",
    "    \n",
    "    max_depth=None,                      # Default: None. Maximum depth of the tree.\n",
    "                                         # - None: Expands nodes until leaves are pure or\n",
    "                                         #         have fewer samples than min_samples_split.\n",
    "    \n",
    "    min_samples_split=2,                 # Default: 2. Minimum samples required to split a node:\n",
    "                                         # - Integer: Exact number (e.g., 2).\n",
    "                                         # - Float: Fraction of total samples (e.g., 0.1 for 10%).\n",
    "    \n",
    "    min_samples_leaf=1,                  # Default: 1. Minimum samples required at a leaf node:\n",
    "                                         # - Integer: Absolute number (e.g., 1).\n",
    "                                         # - Float: Fraction of total samples (e.g., 0.05 for 5%).\n",
    "    \n",
    "\n",
    "    \n",
    "    min_weight_fraction_leaf=0.0,        # Default: 0. Minimum weighted fraction of total weight at a leaf node.\n",
    "                                         # Useful for datasets with sample weights.\n",
    "    \n",
    "    max_features=None,                   # Default: None. Number of features to consider for the best split:\n",
    "                                         # - None: Considers all features.\n",
    "                                         # - \"auto\": Same as None.\n",
    "                                         # - \"sqrt\": Square root of the number of features.\n",
    "                                         # - \"log2\": Logarithm base 2 of the features.\n",
    "                                         # - Integer or float: Exact number or percentage of features.\n",
    "    \n",
    "    random_state=None,                   # Default: None. Seed for random number generator for reproducibility.\n",
    "                                         # - None: Random behavior.\n",
    "                                         # - Integer: Fixed seed for deterministic results.\n",
    "    \n",
    "    max_leaf_nodes=None,                 # Default: None. Maximum number of leaf nodes:\n",
    "                                         # - None: Unlimited number of leaf nodes.\n",
    "    \n",
    "    min_impurity_decrease=0.0,           # Default: 0. Minimum impurity decrease required to split a node.\n",
    "                                         # Helps control overfitting by avoiding unnecessary splits.\n",
    "    \n",
    "    ccp_alpha=0.0                        # Default: 0. Complexity parameter for cost-complexity pruning.\n",
    "                                         # Larger values prune the tree more aggressively.\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], # Split quality measure. \n",
    "    # \"squared_error\" is for regression tasks, \"absolute_error\" is more robust to outliers. \n",
    "    # \"friedman_mse\" optimizes for speed, \"poisson\" is used for count data.\n",
    "\n",
    "    'splitter': ['best', 'random'],  # Determines how to choose the split at each node.\n",
    "    # \"best\" chooses the best split, \"random\" may speed up training by selecting a random split.\n",
    "\n",
    "    'max_depth': [None, 5, 10, 20, 50],  # Maximum depth of the tree. Controls model complexity.\n",
    "    # Small values (e.g., 5) prevent overfitting but might underfit. Large values (e.g., 50) increase complexity and risk overfitting.\n",
    "\n",
    "    'min_samples_split': [2, 5, 10, 20],  # Minimum samples required to split an internal node.\n",
    "    # Small values (e.g., 2) allow the tree to grow deeper but may lead to overfitting. Large values (e.g., 20) result in more generalization.\n",
    "\n",
    "    'min_samples_leaf': [1, 2, 5, 10],  # Minimum samples required at a leaf node.\n",
    "    # Small values (e.g., 1) may cause overfitting, while larger values (e.g., 10) can ensure more stable predictions.\n",
    "\n",
    "    'min_weight_fraction_leaf': [0.0, 0.01, 0.05, 0.1],  # Minimum weighted fraction of samples in a leaf.\n",
    "    # Small values (e.g., 0.0) give more freedom for leaf nodes, while large values (e.g., 0.1) prevent too small leaf nodes.\n",
    "\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2', 0.5, 0.7],  # Number of features considered for a split.\n",
    "    # Small values (e.g., 0.5) increase bias but reduce variance. Large values (e.g., 'auto') lead to more flexible trees, risking overfitting.\n",
    "\n",
    "    'random_state': [42],  # Ensures reproducibility of results.\n",
    "    # Fixed value ensures that results are consistent across runs.\n",
    "\n",
    "    'max_leaf_nodes': [None, 10, 20, 50, 100],  # Maximum number of leaf nodes.\n",
    "    # Small values (e.g., 10) prevent the tree from growing too complex. Large values allow more splits, increasing flexibility.\n",
    "\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.05],  # Minimum impurity decrease for a split.\n",
    "    # Small values (e.g., 0.0) make splits easier to perform, while larger values (e.g., 0.05) result in fewer splits and simpler trees.\n",
    "\n",
    "    'ccp_alpha': [0.0, 0.01, 0.05, 0.1],  # Regularization for pruning to reduce overfitting.\n",
    "    # Small values (e.g., 0.0) allow for more complex trees, while large values (e.g., 0.1) prune aggressively, preventing overfitting.\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtaul_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
