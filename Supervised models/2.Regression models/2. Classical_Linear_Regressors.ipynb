{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install scikit-learn==<version>\n",
    "! pip install scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn  # type: ignore\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Linear Regressors:\n",
    "\n",
    "1. LinearRegression\n",
    "2. SGDRegressor\n",
    "3. Ridge, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  # type: ignore\n",
    "\n",
    "\n",
    "linear_regression = LinearRegression(\n",
    "    fit_intercept=True,      # Whether to include an intercept in the model (True) or force through the origin (False). Default is True.\n",
    "    copy_X=True,             # Whether to copy the input data (True) or modify it in-place (False). Default is True.\n",
    "    n_jobs=-1,               # Number of CPU cores to use for computation (-1 uses all available cores). Default is 1.\n",
    "    positive=False           # Whether to constrain the coefficients to be positive only (True) or allow negative coefficients (False). Default is False.\n",
    ")\n",
    "\n",
    "linear_regression_params = {\n",
    "    \"fit_intercept\": [True, False], \n",
    "    \"copy_X\": [True, False],         \n",
    "    \"n_jobs\": [-1, 0 to n],              \n",
    "    \"positive\": [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "# Get the number of CPU cores\n",
    "import os\n",
    "\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Number of CPU cores: {num_cores}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Regression (Stochastic Gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "\n",
    "\n",
    "sgd_model = sklearn.linear_model.SGDRegressor(\n",
    "    loss=\"squared_error\",         # Loss function to minimize. Options:\n",
    "                                  # - \"squared_error\": Standard linear regression loss (default).\n",
    "                                  # - \"huber\": Robust to outliers.\n",
    "    epsilon=0.1,                  # For \"huber\" loss: Defines the threshold to switch from squared to linear loss.\n",
    "\n",
    "    penalty=\"elasticnet\",         # Regularization type to prevent overfitting:\n",
    "                                  # - \"l2\": Squared penalty on weights (default).\n",
    "                                  # - \"l1\": Absolute penalty (Lasso-style).\n",
    "                                  # - \"elasticnet\": Combination of L1 and L2.\n",
    "                                  # - \"None\": l2\n",
    "\n",
    "    alpha=0.0001,                 # Regularization strength. Larger values mean stricter regularization.\n",
    "    l1_ratio=0.15,                # Ratio of L1 regularization for \"elasticnet\". Default is 0.15.\n",
    "    fit_intercept=True,           # Include an intercept (bias) in the model. Default is True.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    max_iter=1000,                # Maximum number of iterations for convergence. Default is 1000.\n",
    "    tol=0.0001,                   # Tolerance for convergence. Smaller values = higher precision.\n",
    "    shuffle=True,                 # Shuffle training data before each epoch. Improves convergence.\n",
    "    verbose=1,                   # Verbosity level. If set to `1`, the solver will print basic progress information (e.g., iteration count). Higher values give more detailed output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    learning_rate=\"invscaling\",   # Schedule for learning rate adjustment:\n",
    "                                  # - \"constant\": Fixed learning rate.\n",
    "                                  # - \"invscaling\": Decreases as iterations increase (default).\n",
    "                                  # - \"adaptive\": Adjusts based on validation error.\n",
    "    eta0=0.01,                    # Initial learning rate. Default is 0.01.\n",
    "    power_t=0.25,                 # Exponent for inverse scaling learning rate schedule.\n",
    "\n",
    "\n",
    "\n",
    "    early_stopping=False,         # Stop early if validation error stops improving. Default is False.\n",
    "    validation_fraction=0.1,      # Fraction of data used for validation during early stopping. Default is 10%.\n",
    "    n_iter_no_change=5,           # Number of epochs with no improvement before stopping. Default is 5.\n",
    "    \n",
    "    random_state=None            # Controls the randomness for reproducibility. None means random behavior, or set an integer (e.g., 0, 42) for consistent results.\n",
    "    warm_start=False,             # Use previous solution as initialization for new training. Default is False.\n",
    "    average=False                 # If True, averages weights over iterations for stability.\n",
    ")\n",
    "\n",
    "# Parameter options for fine-tuning\n",
    "sgd_params = {\n",
    "    \"loss\": [\"squared_error\", \"huber\", \"epsilon_insensitive\"],\n",
    "    \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    \"l1_ratio\": [0.1, 0.15, 0.5],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"max_iter\": [500, 1000, 5000],\n",
    "    \"tol\": [0.0001, 0.001, 0.01],\n",
    "    \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "    \"eta0\": [0.01, 0.1, 1.0],\n",
    "    \"early_stopping\": [True, False],\n",
    "    \"validation_fraction\": [0.1, 0.2],\n",
    "    \"n_iter_no_change\": [5, 10, 20],\n",
    "    \"random_state\": [None, 42],\n",
    "    \"warm_start\": [True, False],\n",
    "    \"average\": [True, False]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "squared_error       : Ordinary least squares loss; minimizes the squared difference between predicted and actual values.\n",
    "huber               : Combines squared error and absolute error, making it robust to outliers by reducing their influence.\n",
    "epsilon_insensitive : Ignores errors within a margin of epsilon, focusing only on larger deviations.\n",
    "\n",
    "constant            : Keeps the learning rate fixed throughout training.\n",
    "invscaling          : Gradually reduces the learning rate with each iteration using a predefined decay formula.\n",
    "adaptive            : Adjusts the learning rate dynamically based on validation error; reduces it if the model stops improving.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression = Linear regression + L2 Regularization (Uses Closed form solution)\n",
    "\n",
    "\n",
    "import sklearn.linear_model # type: ignore\n",
    "\n",
    "ridge_model= sklearn.linear_model.Ridge(\n",
    "    alpha=1,                    # Regularization strength. Larger values mean more regularization (reducing overfitting), smaller values mean less regularization (more flexible model). Default is 1.\n",
    "    fit_intercept=True,          # Whether to include an intercept term (bias). True means the model will learn the intercept, False means no intercept (useful if data is already centered).\n",
    "    copy_X=True,                 # Whether to copy the input data X. True keeps the original X unchanged, False modifies X in place (saves memory).\n",
    "    max_iter=1000,               # Maximum iterations for iterative solvers. Higher values allow the solver more time to converge. Default is 1000. Use None for no limit.\n",
    "    tol=0.0001,                  # Tolerance for convergence. When the change in the solution becomes smaller than `tol`, the algorithm stops. Lower values mean higher precision.\n",
    "    solver='auto',               # Solver for optimization. 'auto' lets the algorithm choose, or you can specify solvers like 'svd', 'cholesky', etc.\n",
    "    positive=False,              # If True, coefficients are constrained to be non-negative. False allows negative coefficients (useful for financial models).\n",
    "    random_state=None            # Controls the randomness for reproducibility. None means random behavior, or set an integer (e.g., 0, 42) for consistent results.\n",
    ")\n",
    "\n",
    "ridge_regression_params = {\n",
    "    \"alpha\": [0.1, 1, 10],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"copy_X\": [True, False],\n",
    "    \"max_iter\": [100, 500, 1000, 3000, 5000, 7000, 10000, None],\n",
    "    \"tol\": [0.0001],\n",
    "    \"solver\": ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],\n",
    "    \"positive\": [True, False],\n",
    "    \"random_state\": [None, 0, 42]\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    "auto      : Automatically selects the best solver based on the dataset. Typically selects svd for small datasets & saga for large sparse datasets.\n",
    "svd       : Singular Value Decomposition. Best for smaller datasets.\n",
    "cholesky  : Uses Cholesky decomposition to solve linear systems. Suitable for dense matrices.\n",
    "lsqr      : Least Squares QR decomposition. Efficient for sparse data.\n",
    "sparse_cg : Conjugate Gradient solver, designed for sparse matrices.\n",
    "sag       : Stochastic Average Gradient. Fast for large datasets, especially when data is sparse.\n",
    "saga      : Stochastic Average Gradient with improvements. Good for large datasets, especially sparse ones.\n",
    "lbfgs     : Limited-memory Broyden-Fletcher-Goldfarb-Shanno. A quasi-Newton method for optimization, suitable for large datasets.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ridge_regression # type: ignore\n",
    "\n",
    "ridge_regression_model= ridge_regression(\n",
    "    X=\"your_input_matrix\",       # 2D input data matrix\n",
    "    y=\"your_target_array\",       # 1D or 2D output matrix :1D for single-output regression, 2D array for multi-output regression.\n",
    "    alpha=1.0,                   # Regularization strength. Larger values mean more regularization (reducing overfitting), smaller values mean less regularization (more flexible model). Default is 1.\n",
    "    sample_weight=None,          # Sample weights allow you to give different importance to each sample. If None, all samples are treated equally.\n",
    "\n",
    "    verbose=1,                   # Verbosity level. If set to `1`, the solver will print basic progress information (e.g., iteration count). Higher values give more detailed output.\n",
    "    return_n_iter=True,          # Whether to return the number of iterations taken by the solver for convergence. If `True`, the function will return this value, which can be useful for diagnosing convergence issues.\n",
    "    return_intercept=True,       # Whether to return the intercept term (bias) along with the coefficients. If `True`, both the coefficients and intercept will be returned.\n",
    "    check_input=True             # Checks if the input data (X and y) are in the correct format. Helps prevent errors by ensuring valid input.\n",
    "\n",
    "    max_iter=1000,               # Maximum iterations for iterative solvers. Higher values allow the solver more time to converge. Default is 1000. Use None for no limit.\n",
    "    tol=0.0001,                  # Tolerance for convergence. When the change in the solution becomes smaller than `tol`, the algorithm stops. Lower values mean higher precision.\n",
    "    solver='auto',               # Solver for optimization. 'auto' lets the algorithm choose, or you can specify solvers like 'svd', 'cholesky', etc.\n",
    "    positive=False,              # If True, coefficients are constrained to be non-negative. False allows negative coefficients (useful for financial models).\n",
    "    random_state=None            # Controls the randomness for reproducibility. None means random behavior, or set an integer (e.g., 0, 42) for consistent results.\n",
    ")\n",
    "\n",
    "\n",
    "ridge_regression_params = {\n",
    "    \"alpha\": [0.1, 1.0, 10.0],\n",
    "    \"sample_weight\": [None, 0.5, 1.0],\n",
    "    \"max_iter\": [1000, 5000, None],\n",
    "    \"tol\": [0.0001, 0.001, 0.01],\n",
    "    \"solver\": ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'],\n",
    "    \"positive\": [True, False],\n",
    "    \"random_state\": [None, 42, 0],\n",
    "    \"verbose\": [0, 1, 2],\n",
    "    \"return_n_iter\": [True, False],\n",
    "    \"return_intercept\": [True, False],\n",
    "    \"check_input\": [True, False]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Verbosity:\n",
    "verbose=0: No output. The solver runs silently without showing progress.\n",
    "verbose=1: Basic output. It shows a summary of progress (e.g., the number of iterations).\n",
    "verbose=2: Detailed output. It provides more in-depth information, such as the current state of each iteration.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "ridgecv_model = sklearn.linear_model.RidgeCV(\n",
    "    alphas=(0.1, 1.0, 10.0),          # Array of alpha values (default is (0.1, 1.0, 10.0)).\n",
    "    fit_intercept=True,               # Whether to calculate the intercept term (default is True).\n",
    "    scoring=None,                     # Scoring method (default is None,meaning negative mean squared error).\n",
    "    cv=None,                          # Cross-validation strategy (default is None, meaning Leave-One-Out CV).\n",
    "    gcv_mode='auto',                  # Method for Leave-One-Out CV (default is 'auto').\n",
    "    store_cv_results=False,           # Whether to store cross-validation results (default is False).\n",
    "    alpha_per_target=False            # Whether to optimize a separate alpha for each target (default is False).\n",
    ")\n",
    "\n",
    "ridge_cv_params = {\n",
    "    \"alphas\": [(0.1, 1.0, 10.0)],    \n",
    "    \"fit_intercept\": [True, False],  \n",
    "    \"cv\": [None, 5, 10, 'KFold', 'StratifiedKFold'],\n",
    "    \"gcv_mode\": ['auto', 'svd', 'eigen'],\n",
    "    \"store_cv_results\": [True, False], \n",
    "    \"alpha_per_target\": [True, False] \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtaul_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
