{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "1. GaussianProcessClassifier : Gaussian process classification (GPC) based on Laplace approximation.\n",
    "2. GaussianProcessRegressor  :  Gaussian process regression (GPR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "gpc = sklearn.gaussian_process.GaussianProcessClassifier(\n",
    "    kernel=None,                       # Covariance function for the Gaussian Process. Defaults to RBF kernel.\n",
    "    optimizer=\"fmin_l_bfgs_b\",         # Optimization algorithm for kernel hyperparameters. Default is 'fmin_l_bfgs_b'.\n",
    "    n_restarts_optimizer=0,            # Number of optimizer restarts to avoid local minima. Default is 0.\n",
    "    \n",
    "    max_iter_predict=100,              # Maximum iterations for posterior approximation during prediction. Default is 100.\n",
    "    warm_start=False,                  # Reuse previous solution for incremental training. Default is False.\n",
    "    copy_X_train=True,                 # Whether to store a copy of the training data. Default is True.\n",
    "    multi_class=\"one_vs_rest\",         # Strategy for multi-class classification. Default is 'one_vs_rest'.\n",
    "    n_jobs=None,                       # Number of CPU cores to use. Default is None (single core/-1);else define number.\n",
    "    random_state=None,                 # Controls randomness for reproducibility. None means random behavior; set an integer (e.g., 0, 42) for consistent results.\n",
    ")\n",
    "\n",
    "# Hyperparameters for tuning\n",
    "gpc_hyperparameters = {\n",
    "    \"kernel\": [None, sklearn.gaussian_process.kernels.RBF(), sklearn.gaussian_process.kernels.Matern()],\n",
    "    \"optimizer\": [\"fmin_l_bfgs_b\", None],  # Default optimizer or no optimization\n",
    "    \"n_restarts_optimizer\": [0, 5, 10],    # Number of restarts to avoid local minima\n",
    "    \"max_iter_predict\": [50, 100, 200],    # Iterations for posterior approximation\n",
    "    \"warm_start\": [True, False],           # Incremental training or fresh start\n",
    "    \"copy_X_train\": [True, False],         # Copy training data or modify in place\n",
    "    \"multi_class\": [\"one_vs_rest\", \"one_vs_one\"],  # Multi-class classification strategy\n",
    "    \"n_jobs\": [-1, None],                  # Use all cores (-1) or single core (None)\n",
    "    \"random_state\": [None, 42, 0]          # Random state for reproducibility\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process # type: ignore\n",
    "\n",
    "gpr = sklearn.gaussian_process.GaussianProcessRegressor(\n",
    "    kernel=None,                   # Kernel defines the covariance function of the Gaussian Process. \n",
    "                                   # If None, the default kernel is RBF. Custom kernels can encode specific assumptions \n",
    "                                   # about the function being modeled (e.g., smoothness, periodicity, etc.).\n",
    "\n",
    "    alpha=1e-10,                   # Alpha controls the model's sensitivity to noise in the data.\n",
    "                                   # It helps prevent overfitting by adding extra noise to smooth predictions.\n",
    "                                   # A small alpha (e.g., 1e-10) means the model trusts the data with minimal noise assumption.\n",
    "                                   # A larger alpha means the model is more cautious, assuming more noise in the data.\n",
    "                                   # It balances between overfitting (fitting too closely to data) and underfitting (not fitting enough).\n",
    "\n",
    "\n",
    "    optimizer=\"fmin_l_bfgs_b\",     # Optimization algorithm to determine kernel hyperparameters that maximize the \n",
    "                                   # log-marginal likelihood. \"fmin_l_bfgs_b\" is a quasi-Newton method. \n",
    "                                   # Set to None to disable optimization, suitable for fixed kernel parameters.\n",
    "\n",
    "    n_restarts_optimizer=0,        # Number of restarts for the optimizer from different initial points. \n",
    "                                   # Useful for avoiding local minima and finding a globally optimal solution.\n",
    "\n",
    "    normalize_y=False,             # If True, the target values are normalized to zero mean and unit variance before \n",
    "                                   # fitting. Normalization can improve performance when the output has a large variance.\n",
    "\n",
    "    copy_X_train=True,             # If True, the training data is copied, ensuring the original data remains unchanged. \n",
    "                                   # Set to False for memory efficiency when the input data does not need to be preserved.\n",
    "\n",
    "    random_state=None              # Determines reproducibility of random processes (e.g., when \"optimizer\" is \"random\"). \n",
    "                                   # Use an integer for deterministic results or None for stochastic behavior.\n",
    ")\n",
    "\n",
    "# Hyperparameters for tuning\n",
    "gpr_hyperparameters = {\n",
    "    \"kernel\": [\n",
    "        None,\n",
    "        sklearn.gaussian_process.kernels.RBF(),           # Radial Basis Function kernel, assumes smooth functions.\n",
    "        sklearn.gaussian_process.kernels.Matern(),        # Matern kernel, flexible with parameterized smoothness.\n",
    "        sklearn.gaussian_process.kernels.WhiteKernel()    # White noise kernel, adds random noise.\n",
    "    ],\n",
    "    \"alpha\": [1e-10, 1e-5, 0.1, 1.0],  # Range of noise levels for regularization.\n",
    "    \"optimizer\": [\"fmin_l_bfgs_b\", None],  # Standard optimizer or no optimization.\n",
    "    \"n_restarts_optimizer\": [0, 5, 10],  # Number of optimizer restarts.\n",
    "    \"normalize_y\": [True, False],  # Normalize target values or use raw values.\n",
    "    \"copy_X_train\": [True, False],  # Copy training data or allow modifications.\n",
    "    \"random_state\": [None, 42, 0],  # Reproducibility settings.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels and Kernel Suitability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width: 35%; display: inline-block; vertical-align: top;\">\n",
    "  <tr>\n",
    "    <th>Kernel</th>\n",
    "    <th>Hyperparameter</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>CompoundKernel</td>\n",
    "    <td>RBF</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Sum</td>\n",
    "    <td>Matern</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Product</td>\n",
    "    <td>White</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DotProduct</td>\n",
    "    <td>Pairwise</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Constant</td>\n",
    "    <td>RationalQuadratic</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Exponential</td>\n",
    "    <td>ExpSineSquared</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Hyperparameter</td>\n",
    "    <td>Exponential</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"width: 45%; display: inline-block; vertical-align: top;\">\n",
    "  <tr>\n",
    "    <th>Suitability</th>\n",
    "    <th>Kernels</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Classification</td>\n",
    "    <td>RBF, Matern, Dot Product, Exponentiation, Pairwise</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Regression</td>\n",
    "    <td>RBF, Matern, White, RationalQuadratic</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Both</td>\n",
    "    <td>Compound, Sum, Product</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "# The Kernel class serves as a base class for all kernel functions in scikit-learn.\n",
    "# It provides the interface and common functionality for different kernels, allowing the user to build custom kernels by extending this class.\n",
    "# Best suited when you want to define a custom kernel or extend existing kernel functionality to fit your specific use case.\n",
    "\n",
    "kernel = sklearn.gaussian_process.kernels.Kernel()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.gaussian_process.kernels import Kernel # type: ignore\n",
    "import numpy as np\n",
    "\n",
    "class CustomKernel(Kernel):\n",
    "    def __init__(self, param1=1.0):\n",
    "        self.param1 = param1\n",
    "    \n",
    "    def __call__(self, X, Y=None):\n",
    "        return np.exp(-self.param1 * np.linalg.norm(X - Y)**2)  # Implement your custom kernel function here: X and Y are the inputs (could be data points or matrices)\n",
    "\n",
    "    def diag(self, X):\n",
    "        return np.ones(X.shape[0])                               # Implement the diagonal part of the kernel (for computing the diagonal of the kernel matrix).\n",
    "\n",
    "\n",
    "# Example usage of the custom kernel\n",
    "custom_kernel = CustomKernel(param1=2.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In `sklearn`, kernels such as `ConstantKernel` have hyperparameters that can be accessed and adjusted. \n",
    "These hyperparameters define properties like constant value, length scale, etc., for the kernel.\n",
    "We can use the `get_params()` method to access and modify the kernel's hyperparameters, and \n",
    "the `hyperparameters` attribute to directly interact with them.\n",
    "\"\"\"\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel\n",
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "# Create a synthetic dataset for testing\n",
    "X, y = make_friedman2(n_samples=50, noise=0, random_state=0)\n",
    "\n",
    "# Define a kernel with a constant value and its bounds for optimization\n",
    "kernel = ConstantKernel(constant_value=1.0,constant_value_bounds=(0.0, 10.0))\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    "# Access hyperparameters of the kernel\n",
    "for hyperparameter in kernel.hyperparameters:\n",
    "    print(hyperparameter)\n",
    "\n",
    "# Output: Hyperparameter(name='constant_value', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    "# Get all parameters of the kernel\n",
    "params = kernel.get_params()\n",
    "for key in sorted(params):\n",
    "    print(f\"{key} : {params[key]}\")\n",
    "\n",
    "# Output: constant_value : 1.0\n",
    "# constant_value_bounds : (0.0, 10.0)\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "# The CompoundKernel combines multiple kernels together to model complex patterns in data.\n",
    "# It allows using additive or multiplicative combinations of different kernel functions to capture various aspects of the data.\n",
    "\n",
    "# Best suited for datasets with mixed patterns, where different kernel types are needed to capture different features like periodicity, smoothness, and noise.\n",
    "\n",
    "compoundkernel_kernel = sklearn.gaussian_process.kernels.CompoundKernel(\n",
    "    kernel1=None,                        # The first kernel in the compound kernel.\n",
    "                                         # Default: None. This can be any valid kernel (e.g., RBF, Matern, etc.).\n",
    "                                         # Represents the first component in the compound kernel. \n",
    "                                         # Useful for capturing certain patterns in data (e.g., smooth trends with RBF).\n",
    "    \n",
    "    kernel2=None,                        # The second kernel in the compound kernel.\n",
    "                                         # Default: None. This can be any valid kernel.\n",
    "                                         # Represents the second component in the compound kernel. \n",
    "                                         # Useful for capturing different patterns, like periodicity with ExpSineSquared.\n",
    "    \n",
    "    factor=1.0                           # A scaling factor to adjust the combined kernel.\n",
    "                                         # Default: 1.0. Adjusts the strength of the kernel combination.\n",
    "                                         # A factor > 1.0 emphasizes the influence of the kernels, while a value < 1.0 reduces their effect.\n",
    ")\n",
    "\n",
    "\n",
    "CompoundKernel_hyperparameters = {\n",
    "    \"kernel1\": [sklearn.gaussian_process.kernels.RBF(length_scale=1.0), sklearn.gaussian_process.kernels.Matern(length_scale=1.0)],  # First kernel to combine (e.g., RBF or Matern).\n",
    "    \"kernel2\": [sklearn.gaussian_process.kernels.ExpSineSquared(periodicity=1.0), sklearn.gaussian_process.kernels.RationalQuadratic()],  # Second kernel to combine (e.g., ExpSineSquared or RationalQuadratic).\n",
    "    \"factor\": [0.1, 1.0, 10.0],  # Factor controlling the strength of the kernel combination.\n",
    "                                 # Default: 1.0. A larger factor increases the contribution of the kernels, while a smaller factor reduces it.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "\"\"\"\n",
    "The Sum kernel is used to combine multiple kernels by summing their individual outputs. \n",
    "It is useful when the data is believed to have multiple underlying processes, each captured by a different kernel.\n",
    "This allows modeling of complex data patterns that may have both smooth and rough characteristics, or different periodic behaviors.\n",
    "\"\"\"\n",
    "sum_kernel = sklearn.gaussian_process.kernels.Sum(\n",
    "    k1=sklearn.gaussian_process.kernels.RBF(length_scale=1.0),  # The first base-kernel of the sum-kernel. Models smooth variations in data.\n",
    "    k2=sklearn.gaussian_process.kernels.Matern(length_scale=1.0, nu=1.5)  # The second base-kernel of the sum-kernel. Models rougher variations.\n",
    ")\n",
    "\n",
    "\n",
    "Product_hyperparameters = {\n",
    "    \"k1\": [sklearn.gaussian_process.kernels.RBF(length_scale=1.0), sklearn.gaussian_process.kernels.Matern(length_scale=1.0, nu=1.5)],  # The first kernel in the product combination.\n",
    "    \"k2\": [sklearn.gaussian_process.kernels.RBF(length_scale=1.0), sklearn.gaussian_process.kernels.ExpSineSquared(periodicity=1.0)],  # The second kernel in the product combination.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "\"\"\"\n",
    "The Product kernel models interactions between different features of the data by multiplying the results of two other kernels. \n",
    "It is useful when the data is believed to have interactions between different processes or features, where each feature or process is described by its own kernel.\n",
    "\"\"\"\n",
    "product_kernel = sklearn.gaussian_process.kernels.Product(\n",
    "    k1=sklearn.gaussian_process.kernels.RBF(length_scale=1.0),  # The first base-kernel of the product-kernel. It could be any kernel, like RBF, Matern, etc.\n",
    "    k2=sklearn.gaussian_process.kernels.Matern(length_scale=1.0, nu=1.5)  # The second base-kernel of the product-kernel. It models the interaction of the data with the first kernel.\n",
    ")\n",
    "\n",
    "Product_hyperparameters = {\n",
    "    \"k1\": [sklearn.gaussian_process.kernels.RBF(length_scale=1.0), sklearn.gaussian_process.kernels.Matern(length_scale=1.0, nu=1.5)],  # The first kernel in the product combination.\n",
    "    \"k2\": [sklearn.gaussian_process.kernels.RBF(length_scale=1.0), sklearn.gaussian_process.kernels.ExpSineSquared(periodicity=1.0)],  # The second kernel in the product combination.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "\"\"\"\n",
    "The DotProduct kernel represents the inner product of two vectors in a high-dimensional space. \n",
    "This kernel is typically used for modeling data where the relationship between points is based on their similarity in the feature space, without any scaling or transformation.\n",
    "It is useful for linear regression problems or any scenario where you want to model similarity through dot products in the feature space.\n",
    "\"\"\"\n",
    "dot_product_kernel = sklearn.gaussian_process.kernels.DotProduct(\n",
    "    sigma_0=1.0,                  # Controls the variance of the dot product kernel. \n",
    "                                 # Default: 1.0. A larger value scales up the kernel, allowing greater similarity between points.\n",
    "    \n",
    "    sigma_0_bounds=(1e-5, 1e5),   # Tuple specifying the lower and upper bounds for `sigma_0` during hyperparameter optimization.\n",
    "                                 # Default: (1e-5, 1e5). This allows for a wide range of values, ensuring the kernel can adjust to different scales.\n",
    ")\n",
    "\n",
    "DotProduct_hyperparameters = {\n",
    "    \"sigma_0\": [1.0, 0.1, 10.0],   # Controls the variance of the kernel. A smaller value reduces the kernel's influence, while a larger value increases it.\n",
    "    \"sigma_0_bounds\": [(1e-5, 1e2), (1e-2, 1e3)],  # Specifies the range of values `sigma_0` can take during optimization.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "\"\"\"\n",
    "The ConstantKernel is used to model a constant signal with a fixed magnitude across all data points. \n",
    "It is useful for cases where the data is expected to have a constant variance or baseline value, \n",
    "independent of the input features. This kernel is typically used to model the overall scale of the function.\n",
    "\"\"\"\n",
    "constant_kernel = sklearn.gaussian_process.kernels.ConstantKernel(\n",
    "    constant_value=1,  # The constant magnitude of the kernel. Default is 1. It represents the overall variance or scale of the process.\n",
    "    constant_value_bounds=(1e-5, 1e5)  # Tuple specifying the lower and upper bounds for the constant value during hyperparameter optimization.\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage of the ConstantKernel in a Gaussian Process Regressor model.\n",
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "\n",
    "X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n",
    "kernel = RBF() + ConstantKernel(constant_value=2)  # Using the ConstantKernel to model the baseline signal with a magnitude of 2\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=5, random_state=0).fit(X, y)\n",
    "print(gpr.score(X, y))  # Evaluates the model performance\n",
    "gpr.predict(X[:1, :], return_std=True)  # Predicts and returns the standard deviation of the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExpSineSquared kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "\"\"\"\n",
    "The Exponential Sine Squared kernel models periodic data by capturing oscillatory patterns with a specified periodicity and smoothness.\n",
    "\n",
    "The formula for the kernel is:\n",
    "    k(x, x') = exp( - (2 * sin^2( (π * |x - x'|) / periodicity ) ) / length_scale^2 )\n",
    "\n",
    "Where:\n",
    "- k(x, x') is the kernel function that computes the similarity between two input points x and x'.\n",
    "- |x - x'| represents the absolute difference between the input points x and x'.\n",
    "- sin^2( (π * |x - x'|) / periodicity ) captures the periodic nature of the data. The periodicity controls how often the oscillations repeat.\n",
    "- length_scale controls the smoothness of the oscillations. A smaller value results in more tightly fitting periodic variations, while a larger value leads to smoother oscillations.\n",
    "- exp() is the exponential function that ensures the similarity between points decreases as the distance between them increases.\n",
    "\n",
    "Best suited for datasets with strong periodic or seasonal patterns, such as time-series with cycles, biological rhythms,\n",
    "or oscillatory physical processes.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "expsinesquared_kernel = sklearn.gaussian_process.kernels.ExpSineSquared(\n",
    "    length_scale=1,                      # Controls the smoothness of oscillations. \n",
    "                                         # Default: 1. A smaller value results in more tightly fitting periodic variations. \n",
    "                                         # Larger values lead to smoother and more gradual oscillations.\n",
    "    \n",
    "    periodicity=1,                       # Specifies the period of oscillations. \n",
    "                                         # Default: 1. A smaller value results in more frequent oscillations.\n",
    "                                         # Larger values result in longer periodic cycles.\n",
    "    \n",
    "    length_scale_bounds=(1e-5, 1e5),     # Tuple specifying the lower and upper bounds for length_scale during hyperparameter optimization.\n",
    "                                         # Default: (1e-5, 1e5). This wide range allows the optimizer to search for both tight (small) and smooth (large) oscillations.\n",
    "    \n",
    "    periodicity_bounds=(1e-5, 1e5)       # Tuple specifying the lower and upper bounds for periodicity during hyperparameter optimization.\n",
    "                                         # Default: (1e-5, 1e5). This wide range allows the optimizer to search for both short and long periods in the data.\n",
    ")\n",
    "\n",
    "ExpSineSquared_hyperparameters = {\n",
    "    \"length_scale\": [0.1, 1.0, 10.0],  # Controls smoothness of oscillations. Smaller values for sharper changes; larger for smoother patterns.\n",
    "    \"periodicity\": [0.5, 1.0, 5.0],  # Defines the period of oscillations. Smaller values for shorter cycles; larger for longer cycles.\n",
    "    \"length_scale_bounds\": [(1e-5, 1e2), (1e-2, 1e3)],  # Range within which `length_scale` can vary during optimization.\n",
    "    \"periodicity_bounds\": [(1e-5, 1e2), (1e-2, 1e3)],  # Range within which `periodicity` can vary to capture diverse periodic patterns.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponention Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "\"\"\"\n",
    "The Exponentiation kernel is used to apply exponentiation to the result of another kernel. \n",
    "It is useful when you want to control the smoothness of the kernel by modifying its behavior with an exponent.\n",
    "This kernel can model more complex relationships by scaling the output of another kernel to a higher or lower power.\n",
    "\n",
    "The kernel is defined as:\n",
    "\n",
    "k(X, Y) = (k_0(X, Y))^exponent\n",
    "\n",
    "Where k_0 is the base kernel and the exponent controls the magnitude of the kernel's output.\n",
    "\n",
    "This is typically used when the data may require a non-linear transformation of the base kernel, such as scaling the interaction between features.\n",
    "\"\"\"\n",
    "exponentiation_kernel = sklearn.gaussian_process.kernels.Exponentiation(\n",
    "    kernel=sklearn.gaussian_process.kernels.RationalQuadratic(),  # The base kernel which the exponentiation is applied to.\n",
    "    exponent=2  # The exponent to which the output of the base kernel is raised. Default: 2.0.\n",
    ")\n",
    "\n",
    "Exponentiation_hyperparameters = {\n",
    "    \"kernel\": [sklearn.gaussian_process.kernels.RationalQuadratic(), sklearn.gaussian_process.kernels.RBF(length_scale=1.0)],  # Base kernels to which exponentiation can be applied.\n",
    "    \"exponent\": [0.5, 1.0, 2.0, 3.0],  # Exponent values controlling the scale of the kernel output. Default: 2.0.\n",
    "}\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic, Exponentiation\n",
    "\n",
    "X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n",
    "kernel = Exponentiation(RationalQuadratic(), exponent=2)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=5, random_state=0).fit(X, y)\n",
    "print(gpr.score(X, y))  # 0.419...\n",
    "print(gpr.predict(X[:1, :], return_std=True))  # Predicted values with standard deviation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF  kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "\"\"\"\n",
    "The RBF (Radial Basis Function) kernel, also known as the Gaussian kernel, is a popular kernel in Gaussian Processes \n",
    "for modeling smooth and continuous functions. It is defined as:\n",
    "\n",
    "k(x_i, x_j) = exp(-||x_i - x_j||^2 / (2 * length_scale^2))\n",
    "\n",
    "Where:\n",
    "    - ||x_i - x_j|| is the Euclidean distance between the points x_i and x_j.\n",
    "    - length_scale controls the smoothness of the kernel: smaller values make the kernel more sensitive to local changes, \n",
    "      resulting in a less smooth function, while larger values lead to smoother functions.\n",
    "\n",
    "The RBF kernel is widely used for modeling smooth, non-linear relationships in regression and classification tasks.\n",
    "\n",
    "The `length_scale` parameter defines the scale of the kernel. It determines how much influence each data point has on \n",
    "others in the kernel's evaluation. A small length scale means that only nearby points have a significant effect,\n",
    " while a large length scale means that points further apart have a more significant influence.\n",
    "\n",
    "\"\"\"\n",
    "rbf_kernel = sklearn.gaussian_process.kernels.RBF(\n",
    "    length_scale=1.0,  # Controls the smoothness of the function. Smaller values for sharper variations.\n",
    "    length_scale_bounds=(1e-5, 1e5)  # Bounds on the `length_scale` parameter during optimization.\n",
    ")\n",
    "\n",
    "RBF_hyperparameters = {\n",
    "    \"length_scale\": [0.1, 1.0, 10.0],  # Controls the smoothness of the function.\n",
    "    \"length_scale_bounds\": [(1e-5, 1e2), (1e-2, 1e3)],  # Boundaries for hyperparameter optimization.\n",
    "}\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# Load a sample dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Define the kernel\n",
    "kernel = 1.0 * RBF(length_scale=1.0)\n",
    "\n",
    "# Create and fit a Gaussian Process Classifier with the RBF kernel\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(X, y)\n",
    "\n",
    "# Output the score and predicted probabilities\n",
    "print(gpc.score(X, y))  # e.g., 0.9866...\n",
    "print(gpc.predict_proba(X[:2, :]))  # Predicted probabilities for the first two samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matern kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "\"\"\"\n",
    "The Matern kernel is a generalization of the Radial Basis Function (RBF) kernel, defined by a smoothness parameter nu. \n",
    "It is used in Gaussian Processes to model smoothness and regularity of the underlying function. The kernel is defined as:\n",
    "\n",
    "\"\"\"\n",
    "matern_kernel = sklearn.gaussian_process.kernels.Matern(\n",
    "    length_scale=1.0,  # Controls the smoothness of the function. Smaller values result in sharper variations.\n",
    "    length_scale_bounds=(1e-5, 1e5),  # Lower and upper bounds for `length_scale` during optimization.\n",
    "    nu=1.5  # Defines the smoothness of the kernel. Default is 1.5. Can be set to 0.5, 1.5, 2.5, or infinity.\n",
    ")\n",
    "\n",
    "Matern_hyperparameters = {\n",
    "    \"length_scale\": [0.1, 1.0, 10.0],  # Controls smoothness of the kernel. Smaller values for sharper variations.\n",
    "    \"length_scale_bounds\": [(1e-5, 1e2), (1e-2, 1e3)],  # Ranges for hyperparameter optimization.\n",
    "    \"nu\": [0.5, 1.5, 2.5, float('inf')],  # Controls the smoothness. Default: 1.5.\n",
    "}\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "kernel = 1.0 * Matern(length_scale=1.0, nu=1.5)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(X, y)\n",
    "print(gpc.score(X, y))  # 0.9866...\n",
    "print(gpc.predict_proba(X[:2, :]))  # Predicted probabilities for the first two samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "\"\"\"\n",
    "The WhiteKernel is used to model noise in Gaussian Process regression. It represents the noise level as a \n",
    "constant, which is independent of the input data.\n",
    "\n",
    "Mathematically, the White kernel is defined as:     k(x_1, x_2) = noise_level if x_1 == x_2 else 0\n",
    "\n",
    "This means that the kernel value is equal to `noise_level` when both inputs are the same (diagonal elements), \n",
    "and 0 when they are different (off-diagonal elements).\n",
    "\n",
    "The main use-case of this kernel is as part of a sum-kernel, where it captures the noise or variance of the signal, \n",
    "modeled as independently and identically normally distributed (i.i.d.). \n",
    "The parameter `noise_level` controls the variance of the noise.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "white_kernel = sklearn.gaussian_process.kernels.WhiteKernel(\n",
    "    noise_level=1.0,               # Controls the magnitude of the noise in the data. Default: 1.0. \n",
    "                                   # A higher value results in higher noise level, affecting the model's ability to fit the data.\n",
    "    \n",
    "    noise_level_bounds=\"fixed\"     # Specifies the bounds for noise_level during hyperparameter optimization. \n",
    "                                   # Default: \"fixed\", meaning no optimization is applied to the noise_level. \n",
    "                                   # It can also be set as a tuple like (1e-5, 1e5) to allow optimization within a range.\n",
    ")\n",
    "\n",
    "WhiteKernel_hyperparameters = {\n",
    "    \"noise_level\": [0.1, 1.0, 10.0],       # Controls the variance of noise. A smaller value reduces the noise, while a larger value increases it.\n",
    "    \"noise_level_bounds\": [(1e-5, 1e5)],    # A range for optimizing the `noise_level` during model fitting. \n",
    "}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n",
    "kernel = DotProduct() + WhiteKernel(noise_level=0.5)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0).fit(X, y)\n",
    "print(gpr.score(X, y))  # 0.3680...\n",
    "print(gpr.predict(X[:2,:], return_std=True))  # Predicted values with standard deviation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "The Pairwise Kernel allows SVM to compute similarities between data points,\n",
    "enabling it to classify or regress on complex, non-linear datasets by transforming\n",
    "them into a higher-dimensional feature space.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.gaussian_process\n",
    "\n",
    "pairwise_kernel = sklearn.gaussian_process.kernels.PairwiseKernel(\n",
    "    gamma=1.0,                               # Controls the flexibility of the kernel. Smaller values make the kernel more flexible (fitting more noise), while larger values make it more rigid (smoothing).\n",
    "                                             # Should be positive. Default: 1.0.\n",
    "    gamma_bounds=(1e-5, 1e5),                # Lower and upper bounds for gamma during hyperparameter tuning. Default: (1e-5, 1e5).\n",
    "    metric='rbf',                            # Various metrics like 'linear', 'rbf', 'cosine', etc., used to calculate pairwise similarities.\n",
    ")\n",
    "\n",
    "PairwiseKernel_hyperparameters = {\n",
    "    \"gamma\": [0.1, 1.0, 10.0],                   # Controls the scale of the kernel. Smaller values for more flexibility; larger values for more rigidity.\n",
    "    \"metric\": [\"linear\", \"additive_chi2\", \"chi2\", \"poly\", \"polynomial\", \"rbf\", \"laplacian\", \"sigmoid\", \"cosine\"],\n",
    "                                                 # Various metrics like 'linear', 'rbf', 'cosine', etc., used to calculate pairwise similarities.\n",
    "    \"gamma_bounds\": [(1e-5, 1e2), (1e-2, 1e3)],  # Ranges for `gamma` during optimization.\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "1.Linear Kernel                 : For linearly separable data.\n",
    "2.Additive Chi-Square Kernel    : For count or frequency data.\n",
    "3.Chi-Square Kernel             : For count or frequency data, similar to the additive_chi2 but without additive scaling.\n",
    "4.Polynomial Kernel             : When data has polynomial relationships.\n",
    "5.RBF Kernel                    : For non-linear relationships with unknown data distributions.\n",
    "6.Laplacian Kernel              : For emphasizing local similarities and handling sparse data.\n",
    "7.Sigmoid Kernel                : For neural network-like data structures or binary classification.\n",
    "8.Cosine Kernel                 : For measuring vector similarity, especially in text or high-dimensional spaces.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import PairwiseKernel\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "kernel = PairwiseKernel(metric='rbf')  # Use RBF (Radial Basis Function) kernel for pairwise similarity\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(X, y)\n",
    "print(gpc.score(X, y))  # 0.9733...\n",
    "print(gpc.predict_proba(X[:2, :]))  # Predicted probabilities for the first two samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RationalQuadratic Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process\n",
    "\n",
    "\"\"\"\n",
    "The RationalQuadratic kernel is used for modeling data with varying smoothness. It can be seen as a scale mixture\n",
    " of squared exponential kernels, allowing it to model data that exhibits different levels of smoothness at different scales.\n",
    "\n",
    "The kernel is defined as:\n",
    "\n",
    "k(x_i, x_j) = (1 + (d(x_i, x_j)^2) / (2αl^2))^(-α)\n",
    "\n",
    "\n",
    "Where:\n",
    "- alpha is the scale mixture parameter (controls the relative weight of the different scales),\n",
    "- l is the length scale (determines the smoothness of the kernel),\n",
    "- (d(x_i, x_j)) is the Euclidean distance between input points \\(x_i\\) and \\(x_j\\).\n",
    "\n",
    "This kernel is useful for situations where data has long-range correlations but with varying smoothness over different ranges.\n",
    "\n",
    "\"\"\"\n",
    "RationalQuadratic_kernel = sklearn.gaussian_process.kernels.RationalQuadratic(\n",
    "    length_scale=1.0,                  # The length scale controls the smoothness of the kernel. Default is 1.0.\n",
    "                                       # Smaller values lead to more sensitive kernels (fitting noise), larger values lead to smoother models.\n",
    "    \n",
    "    alpha=1.0,                         # The scale mixture parameter controls the contribution of multiple length scales.\n",
    "                                       # Default is 1.0. Higher values allow for more flexibility in modeling varying smoothness in the data.\n",
    "    \n",
    "    length_scale_bounds=(1e-5, 1e5),    # Bounds on the length scale during optimization. Default: (1e-5, 1e5).\n",
    "                                       # You can use this to restrict the search space during hyperparameter tuning.\n",
    "    \n",
    "    alpha_bounds=(1e-5, 1e5)            # Bounds on the alpha parameter during optimization. Default: (1e-5, 1e5).\n",
    "                                       # This allows flexibility during tuning to capture different smoothness levels.\n",
    ")\n",
    "\n",
    "RationalQuadratic_hyperparameters = {\n",
    "    \"length_scale\": [0.1, 1.0, 10.0],  # Length scale influences how smooth the model is. Smaller values make the kernel more sensitive to noise.\n",
    "    \"alpha\": [0.5, 1.0, 2.0],         # Alpha controls how much smoothness is allowed at different length scales.\n",
    "    \"length_scale_bounds\": [(1e-5, 1e2), (1e-2, 1e3)],  # Bounds on the length scale for optimization.\n",
    "    \"alpha_bounds\": [(1e-5, 1e2), (1e-2, 1e3)],        # Bounds on the alpha parameter for optimization.\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "kernel = RationalQuadratic(length_scale=1.0, alpha=1.5)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(X, y)\n",
    "print(gpc.score(X, y))  # Example output: 0.9733...\n",
    "print(gpc.predict_proba(X[:2, :]))  # Predicted probabilities for the first two data points.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtaul_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
