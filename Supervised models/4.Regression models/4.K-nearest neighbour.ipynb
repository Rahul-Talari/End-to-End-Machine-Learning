{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k- Nearest Neighbours\n",
    "\n",
    "\n",
    "k-NN(K-Nearest Neighbours)\n",
    "\n",
    "1. BallTree                        :\n",
    "      BallTree for fast generalized N-point problems, often used for efficient nearest neighbor searches in high-dimensional data.\n",
    "2. KDTree                          :  KDTree for fast generalized N-point problems, suitable for multi-dimensional data and efficient nearest neighbor searches in spatial data.\n",
    "    3. KNeighborsClassifier            :  Classifier implementing the k-nearest neighbors vote, which classifies a data point based on the majority class of its nearest neighbors.\n",
    "4. KNeighborsRegressor             :  Regression based on k-nearest neighbors, predicting continuous values based on the average of the nearest neighbors’ values.\n",
    "5. KNeighborsTransformer           :  Transform X into a (weighted) graph of k-nearest neighbors, used to represent data in a graph form for further analysis or clustering.\n",
    "6. KernelDensity                   :  Kernel Density Estimation, a non-parametric way to estimate the probability density function of a random variable, useful for anomaly detection.\n",
    "7. LocalOutlierFactor              :  Unsupervised Outlier Detection using the Local Outlier Factor (LOF), identifying outliers by comparing the local density of points to their neighbors.\n",
    "    8. NearestCentroid                 :  Nearest centroid classifier, assigns labels based on the distance to the centroids of different classes, often used in multi-class classification problems.\n",
    "9. NearestNeighbors                :  Unsupervised learner for implementing neighbor searches, which finds and retrieves the nearest neighbors for each data point.\n",
    "10. NeighborhoodComponentsAnalysis :  Neighborhood Components Analysis, a dimensionality reduction method that maximizes the classification accuracy by optimizing the neighbor distance.\n",
    "    11. RadiusNeighborsClassifier      :  Classifier implementing a vote among neighbors within a given radius, ideal for situations where neighbors are not uniformly distributed but lie within a specific range.\n",
    "12. RadiusNeighborsRegressor       :  Regression based on neighbors within a fixed radius, predicting continuous values by considering only the nearest neighbors within a specified distance.\n",
    "13. RadiusNeighborsTransformer     :  Transform X into a (weighted) graph of neighbors nearer than a radius, creating graph representations of data points based on proximity.\n",
    "14. kneighbors_graph               :  Compute the (weighted) graph of k-Neighbors for points in X, typically used to visualize and analyze the relationships between data points.\n",
    "15. radius_neighbors_graph         :  Compute the (weighted) graph of Neighbors for points in X, creating a graph of points based on their proximity to others within a specified radius.\n",
    "16. sort_graph_by_row_values       :  Sort a sparse graph such that each row is stored with increasing values, optimizing the graph structure for storage and faster computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbours\n",
    "\n",
    "**k-NN (K-Nearest Neighbours)** is a simple, non-parametric algorithm used for classification, regression, and anomaly detection. It works by finding the 'k' nearest neighbors to a data point and making predictions based on these neighbors.\n",
    "\n",
    "---\n",
    "### 1. **Core Algorithms for Neighbor Search**\n",
    "\n",
    "| Algorithm       | Description                                                                 |\n",
    "|-----------------|-----------------------------------------------------------------------------|\n",
    "| **BallTree**    | Efficient nearest neighbor search for high-dimensional data. Partitions the space using a tree structure, optimizing search performance. |\n",
    "| **KDTree**      | A tree-based method for nearest neighbor search, partitioning data along the axes of the space. Best suited for low- to moderate-dimensional datasets. |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Classification Algorithms**\n",
    "\n",
    "| Algorithm                    | Description                                                                      |\n",
    "|------------------------------|----------------------------------------------------------------------------------|\n",
    "| **KNeighborsClassifier**      | A simple classification algorithm that assigns a class to a data point based on the majority class of its nearest neighbors. |\n",
    "| **RadiusNeighborsClassifier** | A classification algorithm that assigns a class to a data point based on neighbors within a given radius. |\n",
    "| **NearestCentroid**           | Classifies data points based on the proximity to the centroid of each class, using either Euclidean or Manhattan distance. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Regression Algorithms**\n",
    "\n",
    "| Algorithm                   | Description                                                                  |\n",
    "|-----------------------------|------------------------------------------------------------------------------|\n",
    "| **KNeighborsRegressor**      | A regression model that predicts a target value based on the average of the target values of its nearest neighbors. |\n",
    "| **RadiusNeighborsRegressor** | Similar to KNeighborsRegressor, but uses neighbors within a specified radius for prediction. |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Dimensionality Reduction and Transformation Algorithms**\n",
    "\n",
    "| Algorithm                         | Description                                                                 |\n",
    "|-----------------------------------|-----------------------------------------------------------------------------|\n",
    "| **KNeighborsTransformer**         | Transforms the data by embedding the information of the nearest neighbors. |\n",
    "| **RadiusNeighborsTransformer**    | Similar to KNeighborsTransformer but with a radius-based approach to neighbors. |\n",
    "| **NeighborhoodComponentsAnalysis** | A dimensionality reduction technique that optimizes the neighborhood structure for better classification performance. |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Graph Construction Algorithms**\n",
    "\n",
    "| Algorithm                        | Description                                                                      |\n",
    "|----------------------------------|----------------------------------------------------------------------------------|\n",
    "| **kneighbors_graph**             | Builds a graph where nodes are data points and edges represent the nearest neighbors. |\n",
    "| **radius_neighbors_graph**       | Similar to `kneighbors_graph`, but creates edges based on a specified radius for neighbors. |\n",
    "| **sort_graph_by_row_values**     | Sorts a graph’s rows by their values, often used to prioritize certain neighbors. |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Density Estimation and Outlier Detection**\n",
    "\n",
    "| Algorithm                  | Description                                                                 |\n",
    "|----------------------------|-----------------------------------------------------------------------------|\n",
    "| **KernelDensity**           | A method for estimating the probability density function of a dataset, often used for anomaly detection. |\n",
    "| **LocalOutlierFactor**      | Identifies anomalies by measuring the local density deviation of data points. |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **General Neighbor Search Algorithms**\n",
    "\n",
    "| Algorithm            | Description                                                                   |\n",
    "|----------------------|-------------------------------------------------------------------------------|\n",
    "| **NearestNeighbors**  | NearestNeighbors is an unsupervised learning method for efficiently retrieving similar data points (neighbors). It is useful in tasks like recommendation, anomaly detection, and clustering. With search options like BallTree, KDTree, and Brute Force, it offers flexibility for handling various datasets and use cases. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Core Algorithms for Neighbor Search:\n",
    "- BallTree\n",
    "- KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install scikit-learn==<version>\n",
    "! pip install scikit-learn \n",
    "\n",
    "import sklearn  # type: ignore\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BallTree is effective when the data can be represented as dense multi-dimensional vectors (continuous numerical data),\n",
    "and is particularly useful when working with high-dimensional data or large datasets.\n",
    "\"\"\"\n",
    "import sklearn.neighbors\n",
    "\n",
    "\n",
    "balltree_technique = sklearn.neighbors.BallTree(\n",
    "    data= \"Any\",          # Input data points, typically an array of shape (n_samples, n_features).\n",
    "    leaf_size= 40,        # The leaf size affects the speed and memory efficiency of the BallTree. Default is 40.\n",
    "    metric= \"minkowski\",  # The distance metric used to calculate distances between points. Default is Minkowski distance.\n",
    "    sample_weight= None,  # sample_weight parameter alters the distance calculation by giving more importance to certain data points,\n",
    "                          # thereby influencing the selection of neighbors in algorithms like BallTree.\n",
    "                          #  d = sqrt( ((x_q - x_p)^2 / w) + ((y_q - y_p)^2 / w) )\n",
    ")\n",
    "\n",
    "\n",
    "BallTree_hyperparameters = {\n",
    "    \"leaf_size\": [10, 20, 30, 40, 50],  # Controls the number of samples in each leaf. Adjust to optimize speed vs. accuracy.\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\", \"cosine\"],  # The distance metric to use. Options include:\n",
    "                                                                  # - \"minkowski\" (default), a generalization of Euclidean distance.\n",
    "                                                                  # - \"euclidean\", for standard Euclidean distance.\n",
    "                                                                  # - \"manhattan\", for Manhattan (L1) distance.\n",
    "                                                                  # - \"cosine\", for cosine similarity.\n",
    "    \"sample_weight\": [None, [1, 2, 1], [0.5, 1.5, 2]],  # Optional weights for the samples. None means no weighting.\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Sample data points (3 data points in a 2-dimensional space)\n",
    "X = np.array([[0.0, 1.0], [1.0, 0.0], [2.0, 2.0]])\n",
    "\n",
    "# Create BallTree with default parameters\n",
    "balltree_technique = BallTree(X)\n",
    "\n",
    "# Query the nearest neighbors of the point [1.0, 1.0]\n",
    "dist, ind = balltree_technique.query([[1.0, 1.0]], k=2)  # Find 2 nearest neighbors\n",
    "\n",
    "print(\"Indices of nearest neighbors:\", ind)\n",
    "print(\"Distances to nearest neighbors:\", dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "KDTree is another spatial data structure used for efficient nearest-neighbor search. Unlike BallTree,\n",
    "which uses a binary tree structure to organize data points, KDTree builds a tree by recursively splitting\n",
    "the data along the axes (features) in the dataset. It's highly efficient for low to medium-dimensional data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "kdtree_technique=sklearn.neighbors.KDTree(\n",
    "    data= \"Any\",          # Input data points, typically an array of shape (n_samples, n_features).\n",
    "    leaf_size= 40,        # The leaf size affects the speed and memory efficiency of the BallTree. Default is 40.\n",
    "    metric= \"minkowski\",  # The distance metric used to calculate distances between points. Default is Minkowski distance.\n",
    "    sample_weight= None,  # sample_weight parameter alters the distance calculation by giving more importance to certain data points,\n",
    "                          # thereby influencing the selection of neighbors in algorithms like BallTree.\n",
    "                          #  d = sqrt( ((x_q - x_p)^2 / w) + ((y_q - y_p)^2 / w) \n",
    ")\n",
    "BallTree_hyperparameters = {\n",
    "    \"leaf_size\": [10, 20, 30, 40, 50],  # Controls the number of samples in each leaf. Adjust to optimize speed vs. accuracy.\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\", \"cosine\"],  # The distance metric to use. Options include:\n",
    "                                                                  # - \"minkowski\" (default), a generalization of Euclidean distance.\n",
    "                                                                  # - \"euclidean\", for standard Euclidean distance.\n",
    "                                                                  # - \"manhattan\", for Manhattan (L1) distance.\n",
    "                                                                  # - \"cosine\", for cosine similarity.\n",
    "    \"sample_weight\": [None, [1, 2, 1], [0.5, 1.5, 2]],  # Optional weights for the samples. None means no weighting.\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# Sample data points (3 data points in a 2-dimensional space)\n",
    "X = np.array([[0.0, 1.0], [1.0, 0.0], [2.0, 2.0]])\n",
    "\n",
    "# Create BallTree with default parameters\n",
    "balltree_technique = KDTree(X)\n",
    "\n",
    "# Query the nearest neighbors of the point [1.0, 1.0]\n",
    "dist, ind = balltree_technique.query([[1.0, 1.0]], k=2)  # Find 2 nearest neighbors\n",
    "\n",
    "print(\"Indices of nearest neighbors:\", ind)\n",
    "print(\"Distances to nearest neighbors:\", dist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Classification Algorithms:\n",
    "- KNeighborsClassifier\n",
    "- RadiusNeighborsClassifier\n",
    "- NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - KNeighborsClassifier:\n",
    "\"\"\"\n",
    "It used for classification tasks based on the k-nearest neighbors approach.\n",
    "It assigns a class to a data point based on the majority class of its nearest neighbors in the feature space.\n",
    "For radius simple distance metrics were only used.\n",
    "\n",
    "KNeighborsClassifier uses a fixed number of neighbors (k)\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "knn_classifier = sklearn.neighbors.KNeighborsClassifier(\n",
    "    n_neighbors=5,             # Number of neighbors to use for kneighbors queries. Default is 5.\n",
    "    weights=\"uniform\",         # Weight function used in prediction. Default is 'uniform'.\n",
    "                               # Options:\n",
    "                               # - 'uniform': All points are equally weighted.\n",
    "                               # - 'distance': Points are weighted by inverse distance.\n",
    "                               # - Callable: User-defined function that returns weights.\n",
    "                               \n",
    "    algorithm=\"auto\",          # Algorithm to compute nearest neighbors. Default is 'auto'.\n",
    "                               # Options:\n",
    "                               # - 'auto': Automatically chooses the best algorithm.\n",
    "                               # - 'ball_tree': Uses BallTree for nearest neighbors.\n",
    "                               # - 'kd_tree': Uses KDTree for nearest neighbors.\n",
    "                               # - 'brute': Uses brute-force search.\n",
    "\n",
    "    leaf_size=30,              #leaf_size is a parameter that controls how a tree-based algorithm (like BallTree or KDTree) organizes its data.\n",
    "                               #It decides the number of points in each \"leaf\" of the tree, which are the smallest groups of data in the tree.\n",
    "                               # - For large datasets, you can increase it to reduce memory use.\n",
    "                               # - For small datasets, you can decrease it for faster queries.\n",
    "                               # - Default is 30, which works for most datasets.\n",
    "\n",
    "    p=2,                       # Power parameter for Minkowski metric. Default is 2.\n",
    "                               # - p=1: Manhattan distance (L1).\n",
    "                               # - p=2: Euclidean distance (L2).\n",
    "\n",
    "    metric=\"minkowski\",        # Distance metric for computation. Default is 'minkowski'.\n",
    "                               # Supports custom callable metrics or predefined metrics.\n",
    "\n",
    "    metric_params=None,        # Additional parameters for the distance metric. Default is None.\n",
    "    n_jobs=-1                  # Number of CPU cores to use. Default is 1. Set -1 for all cores.\n",
    ")\n",
    "\n",
    "knn_hyperparameters = {\n",
    "    \"n_neighbors\": [3, 5, 10],\n",
    "    \"weights\": [\"uniform\", \"distance\", lambda distances: 1 / distances],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    \"leaf_size\": [10, 30, 50],\n",
    "    \"p\": [1, 2, 3],\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],\n",
    "    \"metric_params\": [None],\n",
    "    \"n_jobs\": [1, -1],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset and split\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train KNN classifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n",
    "\n",
    "# Print accuracy\n",
    "print(f'Accuracy: {neigh.score(X_test, y_test) * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# - RadiusNeighborsClassifier:\n",
    "\n",
    "\"\"\"\n",
    "It is used for classification tasks based on the radius-neighbor approach.\n",
    "It assigns a class to a data point based on the majority class of its neighbors within a specified radius in the feature space.\n",
    "For radius simple distance metrics were only used.\n",
    "\n",
    "\n",
    "while RadiusNeighborsClassifier uses a fixed radius to determine the neighborhood.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "radius_neighbors_classifier = sklearn.neighbors.RadiusNeighborsClassifier(\n",
    "    radius=1.0,                # Radius to consider for neighbors. Default is 1.0.\n",
    "                               # This parameter defines the neighborhood size.\n",
    "                               # - Larger radius includes more neighbors, while a smaller radius includes fewer neighbors.\n",
    "    \n",
    "    weights=\"uniform\",         # Weight function used in prediction. Default is 'uniform'.\n",
    "                               # Options:\n",
    "                               # - 'uniform': All points are equally weighted.\n",
    "                               # - 'distance': Points are weighted by inverse distance.\n",
    "                               # - Callable: User-defined function that returns weights.\n",
    "    \n",
    "    algorithm=\"auto\",          # Algorithm used to compute nearest neighbors. Default is 'auto'.\n",
    "                               # Options:\n",
    "                               # - 'auto': Automatically chooses the best algorithm.\n",
    "                               # - 'ball_tree': Uses BallTree for nearest neighbors.\n",
    "                               # - 'kd_tree': Uses KDTree for nearest neighbors.\n",
    "                               # - 'brute': Uses brute-force search.\n",
    "\n",
    "    leaf_size=30,              # Controls the structure of BallTree or KDTree. Default is 30.\n",
    "                               # - Larger values increase memory usage but reduce query time for large datasets.\n",
    "    \n",
    "    p=2,                       # Power parameter for Minkowski metric. Default is 2.\n",
    "                               # - p=1: Manhattan distance (L1).\n",
    "                               # - p=2: Euclidean distance (L2).\n",
    "    \n",
    "    metric=\"minkowski\",        # Distance metric for computation. Default is 'minkowski'.\n",
    "                               # Supports custom callable metrics or predefined metrics.\n",
    "\n",
    "    metric_params=None,        # Additional parameters for the distance metric. Default is None.\n",
    "    n_jobs=-1                  # Number of CPU cores to use. Default is 1. Set -1 for all cores.\n",
    ")\n",
    "\n",
    "radius_neighbors_hyperparameters = {\n",
    "    \"radius\": [0.5, 1.0, 1.5],\n",
    "    \"weights\": [\"uniform\", \"distance\", lambda distances: 1 / distances],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    \"leaf_size\": [10, 30, 50],\n",
    "    \"p\": [1, 2, 3],\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],\n",
    "    \"metric_params\": [None],\n",
    "    \"n_jobs\": [1, -1],\n",
    "}\n",
    "\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset and split\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train RadiusNeighborsClassifier\n",
    "radius_neighbors = RadiusNeighborsClassifier(radius=1.0).fit(X_train, y_train)\n",
    "\n",
    "# Print accuracy\n",
    "print(f'Accuracy: {radius_neighbors.score(X_test, y_test) * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NearestCentroid\n",
    "\n",
    "\"\"\"\n",
    "It is used for classification tasks where each class is represented by its centroid.\n",
    "Test samples are classified to the class with the nearest centroid, using a specified distance metric.\n",
    "\n",
    "- Calculate Centroids: Compute the centroid (mean/median) for each class.\n",
    "- Measure Distances: Calculate the distance from the new sample to each centroid.\n",
    "- Classify: Assign the sample to the class with the nearest centroid.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "nearest_centroid_classifier = sklearn.neighbors.NearestCentroid(\n",
    "    metric=\"euclidean\",          # Metric to use for distance computation. Default is 'euclidean'.\n",
    "                                 # Options:\n",
    "                                 # - 'euclidean': Uses the arithmetic mean of the class samples.\n",
    "                                 # - 'manhattan': Uses the feature-wise median of the class samples.\n",
    "    \n",
    "    shrink_threshold=None        # Threshold for shrinking centroids to remove features. Default is None.\n",
    "                                 # - If set, features with low variance are discarded.\n",
    ")\n",
    "\n",
    "nearest_centroid_hyperparameters = {\n",
    "    \"metric\": [\"euclidean\", \"manhattan\"],  # Distance metric options\n",
    "    \"shrink_threshold\": [None, 0.01, 0.1],  # Options for feature shrinking\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import numpy as np\n",
    "\n",
    "# Example data: 2D features and class labels\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "# Create and train the NearestCentroid classifier\n",
    "clf = NearestCentroid()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict the class of a new sample\n",
    "print(clf.predict([[-0.8, -1]]))  # Output: [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Regression Algorithms:\n",
    "- KNeighborsRegressor\n",
    "- RadiusNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KneighboursRegressor\n",
    "\"\"\"\n",
    "It is used for regression tasks based on the k-nearest neighbors approach.\n",
    "It predicts the value of a data point by averaging the values of its nearest neighbors.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "kneighbours_regressor = sklearn.neighbors.KNeighborsRegressor(\n",
    "    n_neighbors=5,            # Number of neighbors to use for kneighbors queries. Default is 5.\n",
    "    \n",
    "    weights=\"uniform\",        # Weight function used in prediction. Default is 'uniform'.\n",
    "                             # Options:\n",
    "                             # - 'uniform': All points are equally weighted.\n",
    "                             # - 'distance': Points are weighted by inverse distance.\n",
    "                             # - Callable: User-defined function that returns weights.\n",
    "    \n",
    "    algorithm=\"auto\",         # Algorithm to compute nearest neighbors. Default is 'auto'.\n",
    "                             # Options:\n",
    "                             # - 'auto': Automatically chooses the best algorithm.\n",
    "                             # - 'ball_tree': Uses BallTree for nearest neighbors.\n",
    "                             # - 'kd_tree': Uses KDTree for nearest neighbors.\n",
    "                             # - 'brute': Uses brute-force search.\n",
    "    \n",
    "    leaf_size=30,             # Leaf size for BallTree or KDTree algorithms. Default is 30.\n",
    "    \n",
    "    p=2,                      # Power parameter for Minkowski metric. Default is 2.\n",
    "                             # - p=1: Manhattan distance (L1).\n",
    "                             # - p=2: Euclidean distance (L2).\n",
    "    \n",
    "    metric=\"minkowski\",       # Distance metric for computation. Default is 'minkowski'.\n",
    "                             # You can also use other distance metrics, like 'euclidean', 'manhattan', etc.\n",
    "    \n",
    "    metric_params=None,       # Additional parameters for the distance metric. Default is None.\n",
    "    \n",
    "    n_jobs=None               # Number of CPU cores to use. Default is None.\n",
    ")\n",
    "kneighbours_regressor_hyperparameters = {\n",
    "    \"n_neighbors\": [3, 5, 10],       # Number of neighbors for predictions\n",
    "    \"weights\": [\"uniform\", \"distance\", lambda distances: 1 / distances],  # Weight options\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],  # Algorithm for finding nearest neighbors\n",
    "    \"leaf_size\": [10, 30, 50],       # Size of the tree leaves for BallTree and KDTree\n",
    "    \"p\": [1, 2],                    # Power parameter for distance calculation\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],  # Distance metrics\n",
    "    \"metric_params\": [None],         # Additional metric parameters\n",
    "    \"n_jobs\": [1, -1],               # Number of CPU cores to use\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Example data: Features X and target values y\n",
    "X = [[0], [1], [2], [3]]  # 2D feature array\n",
    "y = [0, 0, 1, 1]          # Target values\n",
    "\n",
    "# Create and train the KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "neigh.fit(X, y)\n",
    "\n",
    "# Predict the value for a new sample\n",
    "prediction = neigh.predict([[1.5]])\n",
    "print(prediction)  # Output: [0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RadiusNeighborsRegresso\n",
    "\n",
    "\"\"\"\n",
    "It is used for regression tasks based on the radius-nearest neighbors approach.\n",
    "It predicts the target value for a data point based on the average (or weighted average) of its neighbors within a specified radius in the feature space.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "radius_neighbors_regressor = sklearn.neighbors.RadiusNeighborsRegressor(\n",
    "    radius=1.0,                 # Radius of the neighborhood for regression. Default is 1.0.\n",
    "                               # Defines the maximum distance for a neighbor to be included.\n",
    "\n",
    "    weights=\"uniform\",          # Weight function used in prediction. Default is 'uniform'.\n",
    "                               # Options:\n",
    "                               # - 'uniform': All neighbors are weighted equally.\n",
    "                               # - 'distance': Neighbors are weighted by their inverse distance.\n",
    "                               # - Callable: A user-defined function that returns weights based on distance.\n",
    "\n",
    "    algorithm=\"auto\",           # Algorithm to compute the nearest neighbors. Default is 'auto'.\n",
    "                               # Options:\n",
    "                               # - 'auto': Automatically selects the best algorithm.\n",
    "                               # - 'ball_tree': Uses BallTree for neighbor computation.\n",
    "                               # - 'kd_tree': Uses KDTree for neighbor computation.\n",
    "                               # - 'brute': Uses brute-force search.\n",
    "\n",
    "    leaf_size=30,               # Leaf size passed to BallTree or KDTree. Default is 30.\n",
    "                               # Affects the speed and memory usage of the tree.\n",
    "\n",
    "    p=2,                        # Power parameter for Minkowski metric. Default is 2.\n",
    "                               # - p=1: Manhattan distance (L1).\n",
    "                               # - p=2: Euclidean distance (L2).\n",
    "\n",
    "    metric=\"minkowski\",         # Distance metric for computation. Default is 'minkowski'.\n",
    "                               # Supports custom callable metrics or predefined metrics.\n",
    "\n",
    "    metric_params=None,         # Additional parameters for the metric function. Default is None.\n",
    "\n",
    "    n_jobs=-1                   # Number of parallel jobs to run for neighbors search. Default is None.\n",
    "                               # -1 uses all available processors.\n",
    ")\n",
    "\n",
    "radius_neighbors_regressor_hyperparameters = {\n",
    "    \"radius\": [0.5, 1.0, 2.0],              # Radius for neighbor search\n",
    "    \"weights\": [\"uniform\", \"distance\"],     # Weighting options\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],  # Algorithms for neighbor computation\n",
    "    \"leaf_size\": [10, 30, 50],              # Size of leaves in the BallTree/KDTree\n",
    "    \"p\": [1, 2, 3],                         # Distance metric power parameter\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],  # Metric options\n",
    "    \"n_jobs\": [1, -1]                       # Parallel jobs\n",
    "}\n",
    "\n",
    "# Example Code\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the RadiusNeighborsRegressor\n",
    "reg = RadiusNeighborsRegressor(radius=1.5, weights=\"distance\").fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = reg.predict(X_test)\n",
    "print(f\"Predictions: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4.Dimensionality Reduction and Transformation Algorithms:\n",
    "- KNeighborsTransformer\n",
    "- RadiusNeighborsTransformer\n",
    "- NeighborhoodComponentsAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- KNeighborsTransformer\n",
    "\n",
    "Transforms data into a graph of k-nearest neighbors, where each node represents a data point and edges denote the nearest neighbors.\n",
    "The resulting matrix includes distances to the k-nearest neighbors (e.g., neighbor1 distance, neighbor2 distance, neighbor3 distance).\n",
    "This graph structure is commonly used in: \n",
    "- anomaly detection \n",
    "- clustering tasks\n",
    "- Recommendation System\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "kneighbors_transformer = sklearn.neighbors.KNeighborsTransformer(\n",
    "    mode=\"distance\",           # Type of returned matrix.\n",
    "                               # - 'connectivity': Returns a binary matrix (1 for connected, 0 otherwise).\n",
    "                               # - 'distance': Returns a matrix with distances between neighbors.\n",
    "                               # Default is 'distance'.\n",
    "    \n",
    "    n_neighbors=5,             # Number of neighbors for each sample. Default is 5.\n",
    "                               # - Includes an extra neighbor when mode == 'distance'.\n",
    "\n",
    "    algorithm=\"auto\",          # Algorithm to compute nearest neighbors. Default is 'auto'.\n",
    "                               # - 'auto': Chooses the best algorithm.\n",
    "                               # - 'ball_tree': Uses BallTree.\n",
    "                               # - 'kd_tree': Uses KDTree.\n",
    "                               # - 'brute': Uses brute-force search.\n",
    "\n",
    "    leaf_size=30,              # Leaf size for BallTree or KDTree. Default is 30.\n",
    "                               # - Controls tree structure and query time.\n",
    "    \n",
    "    metric=\"minkowski\",        # Distance metric. Default is 'minkowski'.\n",
    "                               # - Other options include 'euclidean', 'manhattan', or callable functions.\n",
    "\n",
    "    p=2,                       # Power parameter for Minkowski metric. Default is 2.\n",
    "                               # - p=1: Manhattan distance (L1).\n",
    "                               # - p=2: Euclidean distance (L2).\n",
    "    \n",
    "    metric_params=None,        # Additional parameters for the distance metric. Default is None.\n",
    "    n_jobs=None                # Number of parallel jobs. Default is 1. Set to -1 to use all CPU cores.\n",
    ")\n",
    "\n",
    "kneighbors_transformer_hyperparameters = {\n",
    "    \"mode\": [\"distance\", \"connectivity\"],\n",
    "    \"n_neighbors\": [3, 5, 10],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    \"leaf_size\": [10, 30, 50],\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],\n",
    "    \"p\": [1, 2, 3],\n",
    "    \"metric_params\": [None],\n",
    "    \"n_jobs\": [None, 1, -1],\n",
    "}\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.neighbors import KNeighborsTransformer\n",
    "\n",
    "# Load the dataset\n",
    "X, _ = load_wine(return_X_y=True)\n",
    "\n",
    "# Check dataset shape\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "\n",
    "# Initialize KNeighborsTransformer\n",
    "transformer = KNeighborsTransformer(n_neighbors=5, mode='distance')\n",
    "\n",
    "# Fit and transform the dataset to create a distance graph\n",
    "X_dist_graph = transformer.fit_transform(X)\n",
    "\n",
    "# Check the transformed graph shape\n",
    "print(f'Transformed graph shape: {X_dist_graph.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- RadiusNeighborsTransformer\n",
    "\n",
    "The main difference is that KNeighborsTransformer connects data points based on the k-nearest neighbors (fixed number of neighbors),\n",
    "while RadiusNeighborsTransformer connects points based on a distance threshold (neighbors within a specified radius).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.neighbors import RadiusNeighborsTransformer\n",
    "\n",
    "radiusneighbor_transformer = RadiusNeighborsTransformer(\n",
    "    mode=\"distance\",           # Type of returned matrix ('distance' or 'connectivity')\n",
    "    radius=42.0,               # Radius of neighbors to consider\n",
    "    algorithm=\"auto\",          # Algorithm for computing nearest neighbors ('auto', 'ball_tree', 'kd_tree', 'brute')\n",
    "    leaf_size=30,              # Controls the structure of tree-based algorithms\n",
    "    metric=\"minkowski\",        # Distance metric used ('minkowski', 'euclidean', 'manhattan', etc.)\n",
    "    p=2,                       # Power parameter for Minkowski distance (default is 2 for Euclidean distance)\n",
    "    metric_params=None,        # Optional additional parameters for the metric\n",
    "    n_jobs=None                # Number of CPU cores to use (-1 for using all cores)\n",
    ")\n",
    "radius_neighbors_transformer_hyperparameters = {\n",
    "    \"mode\": [\"distance\", \"connectivity\"],       # Type of matrix returned (default is 'distance')\n",
    "    \"radius\": [0.5, 1.0, 2.0, 5.0, 10.0],       # Radius within which to find neighbors (default is 1.0)\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],  # Algorithm to compute neighbors (default is 'auto')\n",
    "    \"leaf_size\": [10, 30, 50, 100],              # Leaf size for tree-based algorithms (default is 30)\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],  # Distance metric used (default is 'minkowski')\n",
    "    \"p\": [1, 2, 3],                             # Power parameter for Minkowski distance (default is 2)\n",
    "    \"metric_params\": [None],                    # Additional parameters for the distance metric (default is None)\n",
    "    \"n_jobs\": [None, 1, -1],                    # Number of CPU cores to use (default is None)\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.neighbors import RadiusNeighborsTransformer\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load dataset\n",
    "X, _ = load_wine(return_X_y=True)\n",
    "\n",
    "# Create a pipeline with RadiusNeighborsTransformer and DBSCAN clustering\n",
    "estimator = make_pipeline(\n",
    "    RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n",
    "    DBSCAN(eps=25.0, metric='precomputed')\n",
    ")\n",
    "\n",
    "# Apply clustering\n",
    "X_clustered = estimator.fit_predict(X)\n",
    "\n",
    "# Count the samples in each cluster\n",
    "import numpy as np\n",
    "clusters, counts = np.unique(X_clustered, return_counts=True)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Introduction to NCA:\n",
    "     NCA is a dimensionality reduction technique used to optimize data for distance-based classifiers, like K-Nearest Neighbors (KNN).\n",
    "\n",
    "2. Key Objective:\n",
    "    Unlike traditional methods, NCA doesn't just reduce dimensions; it learns a \n",
    "    linear transformation of the data to improve classification performance.\n",
    "\n",
    "3.Transformation Goal:\n",
    "    It maps data so points of the same class are closer and different classes are farther apart.\n",
    "\n",
    "4.KNN Optimization:\n",
    "    By optimizing the feature space for KNN, NCA makes it easier for KNN to correctly classify points.\n",
    "\n",
    "5.Optimization Process:\n",
    "    It iteratively adjusts the transformation matrix to maximizing the probability that the nearest neighbors of a point belong to the same class.\n",
    "\n",
    "6.Benefits:\n",
    "    This makes NCA particularly useful when working with high-dimensional data, enhancing KNN’s performance by refining distance metrics.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "\n",
    "nca = NeighborhoodComponentsAnalysis(\n",
    "    n_components=2,  # Hyperparameter: Preferred dimensionality of the projected space. \n",
    "                     # If None, it will be set to n_features. \n",
    "                     # Here, it is set to 2, meaning we want the data projected into 2D space.\n",
    "    \n",
    "    init='auto',  # Hyperparameter: Initialization method for the linear transformation.\n",
    "                  # Options: \n",
    "                  # - 'auto': Uses a reasonable initialization based on n_components.\n",
    "                  # - 'pca': Uses principal components of the input data.\n",
    "                  # - 'lda': Uses discriminative components (only applicable if class labels are provided).\n",
    "                  # - 'identity': Uses the identity matrix (works best when n_components <= n_features).\n",
    "                  # - 'random': Initializes with random values from a normal distribution.\n",
    "    \n",
    "    max_iter=100,  # Hyperparameter: Maximum number of iterations for the optimization process.\n",
    "                   # If the algorithm doesn't converge within these iterations, it stops.\n",
    "                   # The default is 50, but we set it to 100 to give the algorithm more iterations to converge.\n",
    "\n",
    "    tol=1e-5,  # Hyperparameter: Convergence tolerance.\n",
    "               # If the improvement in the objective function is less than this value, the optimization stops.\n",
    "               # The default is 1e-5, which we retain here to ensure convergence is achieved.\n",
    "\n",
    "    random_state=42,  # Hyperparameter: Random seed for reproducibility.\n",
    "                      # Setting a fixed value (like 42) ensures the same result if the code is run multiple times.\n",
    "    \n",
    "    verbose=1  # Hyperparameter: Verbosity level for progress messages.\n",
    "               # - 0: No progress messages.\n",
    "               # - 1: Prints progress messages for each iteration.\n",
    "               # - >1: Prints more detailed progress information.\n",
    "               # Here, it is set to 1 to display progress messages during the fitting process.\n",
    ")\n",
    "\n",
    "neighborhood_components_analysis_hyperparameters = {\n",
    "    \"n_components\": [None, 2, 3, 5, 10],\n",
    "    \"init\": [\"auto\", \"pca\", \"lda\", \"identity\", \"random\"],\n",
    "    \"warm_start\": [True, False], \n",
    "    \"max_iter\": [50, 100, 200, 500],\n",
    "    \"tol\": [1e-5, 1e-4, 1e-3],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis, KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset and split into train and test sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.7, random_state=42)\n",
    "\n",
    "# NCA model with hyperparameters\n",
    "nca = NeighborhoodComponentsAnalysis(n_components=3)\n",
    "nca.fit(X_train, y_train)\n",
    "\n",
    "# KNN model with hyperparameters\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(nca.transform(X_train), y_train)  # KNN with NCA transformed data\n",
    "\n",
    "# Test accuracy with NCA + KNN\n",
    "print(f\"Test accuracy using NCA + KNN: {knn.score(nca.transform(X_test), y_test):.4f}\")\n",
    "\n",
    "# Fit KNN on original data and test accuracy\n",
    "knn.fit(X_train, y_train)  # KNN without NCA\n",
    "print(f\"Test accuracy using KNN without NCA: {knn.score(X_test, y_test):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Graph Construction Algorithms:\n",
    "- kneighbors_graph\n",
    "- radius_neighbors_graph\n",
    "- sort_graph_by_row_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "This function is often used in clustering, classification, and graph-based algorithms where understanding\n",
    "the relationships between samples is crucial.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "graph = sklearn.neighbors.kneighbors_graph(\n",
    "    X,               # Input parameter: Dataset for which the neighbors graph is to be constructed.\n",
    "    n_neighbors=5,  # Hyperparameter: Number of neighbors to consider for each sample.\n",
    "                    # Higher values result in denser graphs, while lower values create sparser ones.\n",
    "                    # Here, it is set to 5, meaning each node (sample) will connect to its 5 nearest neighbors.\n",
    "\n",
    "    mode='connectivity',  # Hyperparameter: Type of graph to construct.\n",
    "                          # - 'connectivity': Returns a binary adjacency matrix (1 if a neighbor, 0 otherwise).\n",
    "                          # - 'distance': Returns a matrix with distances to neighbors as edge weights.\n",
    "                          # Here, it is set to 'connectivity' to create a binary adjacency matrix.\n",
    "\n",
    "    metric='minkowski',  # Hyperparameter: Metric used to compute distances between samples.\n",
    "                         # Common options:\n",
    "                         # - 'euclidean': Standard distance (equivalent to minkowski with p=2).\n",
    "                         # - 'manhattan': L1 distance (minkowski with p=1).\n",
    "                         # - 'cosine', 'precomputed', or other valid options from scipy's distance metrics.\n",
    "                         # Here, it is set to 'minkowski', allowing customization through the `p` parameter.\n",
    "\n",
    "    p=2,  # Hyperparameter: Power parameter for the Minkowski metric.\n",
    "          # - p=1: Equivalent to Manhattan distance (L1 norm).\n",
    "          # - p=2: Equivalent to Euclidean distance (L2 norm).\n",
    "          # - Any positive p allows for generalized Minkowski distances.\n",
    "          # Here, it is set to 2 for standard Euclidean distance.\n",
    "\n",
    "    include_self=False,  # Hyperparameter: Determines whether each sample connects to itself.\n",
    "                         # - True: Each sample includes a self-loop.\n",
    "                         # - False: Self-loops are excluded from the graph.\n",
    "                         # - 'auto': Includes self-loops for 'connectivity', excludes them for 'distance'.\n",
    "                         # Here, it is set to False to exclude self-loops.\n",
    "\n",
    "    n_jobs=-1  # Hyperparameter: Number of parallel jobs for computing the neighbors graph.\n",
    "               # - None: Single-threaded processing.\n",
    "               # - -1: Utilizes all available processors.\n",
    "               # - Positive int: Specifies the number of parallel threads.\n",
    "               # Here, it is set to -1 to leverage all CPU cores.\n",
    ")\n",
    "kneighbors_graph_hyperparameters = {\n",
    "    \"n_neighbors\": [3, 5, 10, 20],  # Number of neighbors to consider.\n",
    "    \"mode\": [\"connectivity\", \"distance\"],  # Type of graph (binary adjacency or distance-weighted).\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\", \"cosine\"],  # Distance metric.\n",
    "    \"p\": [1, 2, 3],  # Minkowski power parameter.\n",
    "    \"include_self\": [True, False],  # Whether to include self-loops.\n",
    "    \"n_jobs\": [-1, None],  # Parallelization options.\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import numpy as np\n",
    "\n",
    "# Create a random dataset\n",
    "X = np.random.rand(5, 2)\n",
    "\n",
    "# Compute the connectivity graph with 3 neighbors\n",
    "connectivity_graph = kneighbors_graph(X, n_neighbors=3, mode='connectivity')\n",
    "print(\"Connectivity Graph:\\n\", connectivity_graph.toarray())\n",
    "\n",
    "# Compute the distance graph with 3 neighbors\n",
    "distance_graph = kneighbors_graph(X, n_neighbors=3, mode='distance')\n",
    "print(\"Distance Graph:\\n\", distance_graph.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import radius_neighbors_graph\n",
    "\n",
    "graph = radius_neighbors_graph(\n",
    "    X,  # Input parameter: Dataset for which the neighbors graph is to be constructed.\n",
    "       # X must be of shape (n_samples, n_features), representing the samples and their features.\n",
    "       # It can also be a sparse matrix or a precomputed structure like BallTree.\n",
    "\n",
    "    radius=1.0,  # Hyperparameter: The radius within which neighbors will be considered.\n",
    "                 # A larger radius means each sample will connect to more neighbors. \n",
    "                 # Here, it is set to 1.0, meaning each point will be connected to its neighbors within a radius of 1.\n",
    "\n",
    "    mode='connectivity',  # Hyperparameter: Type of graph to construct.\n",
    "                          # - 'connectivity': Returns a binary adjacency matrix (1 if a neighbor is within the radius, 0 otherwise).\n",
    "                          # - 'distance': Returns a matrix with distances to neighbors as edge weights.\n",
    "                          # Here, it is set to 'connectivity' to create a binary adjacency matrix.\n",
    "\n",
    "    metric='minkowski',  # Hyperparameter: Metric used to compute distances between samples.\n",
    "                         # Common options:\n",
    "                         # - 'euclidean': Standard distance (equivalent to Minkowski with p=2).\n",
    "                         # - 'manhattan': L1 distance (Minkowski with p=1).\n",
    "                         # - 'cosine', 'precomputed', or other valid options from scipy's distance metrics.\n",
    "                         # Here, it is set to 'minkowski', allowing customization through the `p` parameter.\n",
    "\n",
    "    p=2,  # Hyperparameter: Power parameter for the Minkowski metric.\n",
    "          # - p=1: Equivalent to Manhattan distance (L1 norm).\n",
    "          # - p=2: Equivalent to Euclidean distance (L2 norm).\n",
    "          # - Any positive p allows for generalized Minkowski distances.\n",
    "          # Here, it is set to 2 for standard Euclidean distance.\n",
    "\n",
    "    include_self=False,  # Hyperparameter: Determines whether each sample connects to itself.\n",
    "                         # - True: Each sample includes a self-loop.\n",
    "                         # - False: Self-loops are excluded from the graph.\n",
    "                         # - 'auto': Includes self-loops for 'connectivity', excludes them for 'distance'.\n",
    "                         # Here, it is set to False to exclude self-loops.\n",
    "\n",
    "    n_jobs=-1  # Hyperparameter: Number of parallel jobs for computing the neighbors graph.\n",
    "               # - None: Single-threaded processing.\n",
    "               # - -1: Utilizes all available processors.\n",
    "               # - Positive int: Specifies the number of parallel threads.\n",
    "               # Here, it is set to -1 to leverage all CPU cores.\n",
    ")\n",
    "radius_neighbors_graph_hyperparameters = {\n",
    "    \"radius\": [0.1, 0.5, 1.0, 2.0],  # Radius to define the neighborhood for each sample.\n",
    "    \"mode\": [\"connectivity\", \"distance\"],  # Type of graph (binary adjacency or distance-weighted).\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\", \"cosine\"],  # Distance metric.\n",
    "    \"p\": [1, 2, 3],  # Minkowski power parameter.\n",
    "    \"include_self\": [True, False],  # Whether to include self-loops.\n",
    "    \"n_jobs\": [-1, None],  # Parallelization options.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import sort_graph_by_row_values\n",
    "\n",
    "graph = sort_graph_by_row_values(\n",
    "    graph,  # Input parameter: The graph (adjacency matrix) to be sorted.\n",
    "           # graph should be a sparse matrix (spmatrix) representing the neighbors graph, \n",
    "           # where each row corresponds to a sample and the columns represent the neighbors or connections.\n",
    "\n",
    "    copy=False,  # Hyperparameter: Whether to return a copy of the matrix or modify it in-place.\n",
    "                 # - True: A new sorted graph is returned, and the original graph remains unchanged.\n",
    "                 # - False: The original graph is sorted in-place, saving memory.\n",
    "                 # Here, it is set to False to avoid creating a new matrix and modify the existing one.\n",
    "\n",
    "    warn_when_not_sorted=True  # Hyperparameter: Whether to issue a warning if the graph is already sorted.\n",
    "                              # - True: If the graph is already sorted by row values, a warning will be issued.\n",
    "                              # - False: No warning will be issued.\n",
    "                              # Here, it is set to True to alert the user when the graph is already sorted.\n",
    ")\n",
    "sort_graph_by_row_values_hyperparameters = {\n",
    "    \"copy\": [True, False],  # Whether to return a new sorted graph or modify the original.\n",
    "    \"warn_when_not_sorted\": [True, False],  # Whether to issue a warning when the graph is already sorted.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Density Estimation and Outlier Detection:\n",
    "- KernelDensity\n",
    "- LocalOutlierFactor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "kerneldensity = KernelDensity(\n",
    "    bandwidth=1.0,  # Hyperparameter: Bandwidth of the kernel.\n",
    "                    # - If a float, defines the bandwidth explicitly.\n",
    "                    # - If \"scott\" or \"silverman\", uses these methods to estimate the bandwidth.\n",
    "                    # Here, it is set to 1.0 for direct control over the kernel smoothing.\n",
    "\n",
    "    algorithm=\"auto\",  # Hyperparameter: Tree-based algorithm for computation.\n",
    "                       # - \"auto\": Automatically chooses the best algorithm for the data.\n",
    "                       # - \"kd_tree\": Uses the k-d tree data structure.\n",
    "                       # - \"ball_tree\": Uses the ball tree data structure.\n",
    "                       # Here, it is set to \"auto\" to allow the system to determine the optimal method.\n",
    "\n",
    "    kernel=\"gaussian\",  # Hyperparameter: The kernel used for density estimation.\n",
    "                        # Options:\n",
    "                        # - \"gaussian\": Gaussian kernel (default, smooth and commonly used).\n",
    "                        # - \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\": Other kernel options with different shapes.\n",
    "                        # Here, \"gaussian\" is chosen for smooth density estimation.\n",
    "\n",
    "    metric=\"euclidean\",  # Hyperparameter: Metric for distance computation.\n",
    "                         # - Default is \"euclidean\" for standard L2 distance.\n",
    "                         # - Other metrics can be used as supported by `BallTree` and `KDTree`.\n",
    "                         # Note: The density output normalization is accurate only for the Euclidean metric.\n",
    "\n",
    "    atol=0,  # Hyperparameter: Absolute tolerance for the result.\n",
    "             # - A larger value increases computation speed but reduces precision.\n",
    "             # - Here, it is set to 0 for exact computations.\n",
    "\n",
    "    rtol=0,  # Hyperparameter: Relative tolerance for the result.\n",
    "             # - A larger value increases computation speed but reduces precision.\n",
    "             # - Here, it is set to 0 for exact computations.\n",
    "\n",
    "    breadth_first=True,  # Hyperparameter: Determines the traversal strategy.\n",
    "                         # - True: Uses breadth-first traversal (default, better for larger datasets).\n",
    "                         # - False: Uses depth-first traversal.\n",
    "\n",
    "    leaf_size=40,  # Hyperparameter: Leaf size for the underlying tree.\n",
    "                   # - Controls the number of points at which a tree node is split.\n",
    "                   # - Smaller values increase accuracy but reduce computational efficiency.\n",
    "                   # Here, it is set to 40 as a trade-off.\n",
    "\n",
    "    metric_params=None  # Hyperparameter: Additional parameters for the distance metric.\n",
    "                        # - Passes custom settings for the metric computation.\n",
    "                        # - Default is None, meaning no additional parameters are specified.\n",
    ")\n",
    "kernel_density_hyperparameters = {\n",
    "    \"bandwidth\": [0.1, 0.5, 1.0, 2.0, \"scott\", \"silverman\"],  # Kernel bandwidth options.\n",
    "    \"algorithm\": [\"auto\", \"kd_tree\", \"ball_tree\"],  # Algorithm choices.\n",
    "    \"kernel\": [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"],  # Kernel options.\n",
    "    \"metric\": [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"],  # Distance metrics.\n",
    "    \"atol\": [0, 1e-3, 1e-2, 1e-1],  # Absolute tolerance for faster execution.\n",
    "    \"rtol\": [0, 1e-3, 1e-2, 1e-1],  # Relative tolerance for faster execution.\n",
    "    \"breadth_first\": [True, False],  # Traversal strategies.\n",
    "    \"leaf_size\": [20, 30, 40, 50],  # Leaf size for tree-based methods.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Initialize the LocalOutlierFactor model for anomaly detection\n",
    "localoutlierfactor_model = LocalOutlierFactor(\n",
    "    n_neighbors=20,  # Number of neighbors to use for the local density estimation\n",
    "    metric='minkowski',  # The distance metric to use for the nearest neighbors; 'minkowski' is the default\n",
    "\n",
    "    algorithm='auto',  # Algorithm to use for nearest neighbor search, 'auto' chooses the best one based on the data\n",
    "    leaf_size=30,  # Leaf size parameter for the tree-based algorithms (like BallTree or KDTree)\n",
    "    p=2,  # The power parameter for the Minkowski distance; p=2 corresponds to the Euclidean distance\n",
    "    metric_params=None,  # Additional parameters for the metric function (if applicable)\n",
    "    contamination='auto',  # Proportion of outliers in the data; 'auto' means it will be estimated automatically\n",
    "    novelty=False,  # Whether to use the model for novelty detection (outlier detection in new data)\n",
    "    n_jobs=None  # Number of CPU cores to use for computation; None means using all available cores\n",
    ")\n",
    "\n",
    "\n",
    "{\n",
    "    'n_neighbors': [5, 10, 20, 50],  # Number of neighbors to use for local density estimation\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm used to compute the nearest neighbors\n",
    "    'leaf_size': [10, 30, 50],  # Leaf size for the tree-based algorithms (BallTree or KDTree)\n",
    "    'metric': ['minkowski', 'euclidean', 'manhattan', 'chebyshev'],  # Distance metric used for neighbor search\n",
    "    'p': [1, 2],  # Power parameter for the Minkowski metric. p=1 is equivalent to Manhattan distance, p=2 to Euclidean distance\n",
    "    'contamination': ['auto', 0.05, 0.1, 0.15, 0.2],  # Proportion of outliers in the dataset\n",
    "    'novelty': [True, False],  # If True, the model is used for novelty detection, False for outlier detection\n",
    "    'n_jobs': [-1, 1, 2]  # The number of jobs to run in parallel (-1 for all available CPUs)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7.General Neighbor Search Algorithms:\n",
    "- NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    \n",
    "The NearestNeighbors algorithm in sklearn is a powerful tool for unsupervised learning, designed to find the \n",
    "nearest neighbors of a data point efficiently. It supports various distance metrics and algorithms for flexibility and performance.\n",
    "\"\"\"\n",
    "# KneighboursRegressor\n",
    "\"\"\"\n",
    "It is used for regression tasks based on the k-nearest neighbors approach.\n",
    "It predicts the value of a data point by averaging the values of its nearest neighbors.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "nearestneighbors_technique = sklearn.neighbors.NearestNeighbors(\n",
    "    n_neighbors=5,            # Number of neighbors to use for kneighbors queries. Default is 5.\n",
    "    \n",
    "    radius=1.0,               # Radius of parameter space for radius_neighbors queries. Default is 1.0.\n",
    "    \n",
    "    algorithm=\"auto\",         # Algorithm to compute nearest neighbors. Default is 'auto'.\n",
    "                              # Options:\n",
    "                              # - 'auto': Automatically chooses the best algorithm.\n",
    "                              # - 'ball_tree': Uses BallTree for neighbor search.\n",
    "                              # - 'kd_tree': Uses KDTree for neighbor search.\n",
    "                              # - 'brute': Uses brute-force search.\n",
    "    \n",
    "    leaf_size=30,             # Leaf size for BallTree or KDTree algorithms. Default is 30.\n",
    "    \n",
    "    metric=\"minkowski\",       # Distance metric for computation. Default is 'minkowski'.\n",
    "                              # Other options include 'euclidean', 'manhattan', or a custom callable.\n",
    "    \n",
    "    p=2,                      # Power parameter for Minkowski metric. Default is 2.\n",
    "                              # - p=1: Manhattan distance (L1).\n",
    "                              # - p=2: Euclidean distance (L2).\n",
    "    \n",
    "    metric_params=None,       # Additional parameters for the distance metric. Default is None.\n",
    "    \n",
    "    n_jobs=None               # Number of CPU cores to use. Default is None.\n",
    "                              # -1 means using all available cores.\n",
    ")\n",
    "\n",
    "nearestneighbors_hyperparameters = {\n",
    "    \"n_neighbors\": [3, 5, 10],                               # Number of neighbors for kneighbors queries.\n",
    "    \"radius\": [0.5, 1.0, 2.0],                               # Radius for radius_neighbors queries.\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],  # Algorithms for neighbor search.\n",
    "    \"leaf_size\": [10, 30, 50],                               # Size of the tree leaves for BallTree and KDTree.\n",
    "    \"metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],       # Distance metrics.\n",
    "    \"p\": [1, 2],                                             # Power parameter for Minkowski metric.\n",
    "    \"n_jobs\": [1, -1]                                        # Number of CPU cores to use.\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n",
    "neigh.fit(samples)\n",
    "\n",
    "kneighbors = neigh.kneighbors([[0, 0, 1.3]], n_neighbors=2, return_distance=False)\n",
    "print(\"K-nearest neighbors:\", kneighbors)\n",
    "\n",
    "radius_neighbors = neigh.radius_neighbors([[0, 0, 1.3]], radius=0.4, return_distance=False)\n",
    "print(\"Radius neighbors:\", radius_neighbors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtaul_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
